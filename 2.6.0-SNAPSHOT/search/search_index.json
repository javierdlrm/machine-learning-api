{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Hopsworks Model Management # HSML is the library to interact with the Hopsworks Model Registry and Model Serving. The library makes it easy to export, manage and deploy models. The library automatically configures itself based on the environment it is run. However, to connect from an external Python environment additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section. Getting Started On Hopsworks # Instantiate a connection and get the project model registry and serving handles import hsml # Create a connection connection = hsml . connection () # Get the model registry handle for the project's model registry mr = connection . get_model_registry () # Get the model serving handle for the current model registry ms = connection . get_model_serving () Create a new model model = mr . tensorflow . create_model ( name = \"mnist\" , version = 1 , metrics = { \"accuracy\" : 0.94 }, description = \"mnist model description\" ) model . save ( \"/tmp/model_directory\" ) # or /tmp/model_file Download a model model = mr . get_model ( \"mnist\" , version = 1 ) model_path = model . download () Delete a model model . delete () Get best performing model best_model = mr . get_best_model ( 'mnist' , 'accuracy' , 'max' ) Deploy a model deployment = model . deploy () Start a deployment deployment . start () Make predictions with a deployed model data = { \"instances\" : model . input_example } predictions = deployment . predict ( data ) You can find more examples on how to use the library in examples.hopsworks.ai . Documentation # Documentation is available at Hopsworks Model Management Documentation . Issues # For general questions about the usage of Hopsworks Machine Learning please open a topic on Hopsworks Community . Please report any issue using Github issue tracking . Contributing # If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Introduction"},{"location":"#hopsworks-model-management","text":"HSML is the library to interact with the Hopsworks Model Registry and Model Serving. The library makes it easy to export, manage and deploy models. The library automatically configures itself based on the environment it is run. However, to connect from an external Python environment additional connection information, such as host and port, is required. For more information about the setup from external environments, see the setup section.","title":"Hopsworks Model Management"},{"location":"#getting-started-on-hopsworks","text":"Instantiate a connection and get the project model registry and serving handles import hsml # Create a connection connection = hsml . connection () # Get the model registry handle for the project's model registry mr = connection . get_model_registry () # Get the model serving handle for the current model registry ms = connection . get_model_serving () Create a new model model = mr . tensorflow . create_model ( name = \"mnist\" , version = 1 , metrics = { \"accuracy\" : 0.94 }, description = \"mnist model description\" ) model . save ( \"/tmp/model_directory\" ) # or /tmp/model_file Download a model model = mr . get_model ( \"mnist\" , version = 1 ) model_path = model . download () Delete a model model . delete () Get best performing model best_model = mr . get_best_model ( 'mnist' , 'accuracy' , 'max' ) Deploy a model deployment = model . deploy () Start a deployment deployment . start () Make predictions with a deployed model data = { \"instances\" : model . input_example } predictions = deployment . predict ( data ) You can find more examples on how to use the library in examples.hopsworks.ai .","title":"Getting Started On Hopsworks"},{"location":"#documentation","text":"Documentation is available at Hopsworks Model Management Documentation .","title":"Documentation"},{"location":"#issues","text":"For general questions about the usage of Hopsworks Machine Learning please open a topic on Hopsworks Community . Please report any issue using Github issue tracking .","title":"Issues"},{"location":"#contributing","text":"If you would like to contribute to this library, please see the Contribution Guidelines .","title":"Contributing"},{"location":"CONTRIBUTING/","text":"Python development setup # Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Model Registry uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsml black hsml Python documentation # We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Model registry entity engine methods (e.g. ModelEngine etc.) only require a single line docstring. REST Api implementations (e.g. ModelApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults. Setup and Build Documentation # We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/logicalclocks/keras-autodoc@split-tags-properties Install HSML with docs extras: pip install -e . [ dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py Option 1: Build only current version of docs # Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve Option 2 (Preferred): Build multi-version doc with mike # Versioning on docs.hopsworks.ai # On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsml corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 . Build Instructions # For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ] Adding new API documentation # To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsml.connection.Connection.connection\" , \"hsml.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Contributing"},{"location":"CONTRIBUTING/#python-development-setup","text":"Fork and clone the repository Create a new Python environment with your favourite environment manager, e.g. virtualenv or conda Install repository in editable mode with development dependencies: cd python pip install -e \".[dev]\" Install pre-commit and then activate its hooks. pre-commit is a framework for managing and maintaining multi-language pre-commit hooks. The Model Registry uses pre-commit to ensure code-style and code formatting through black and flake8 . Run the following commands from the python directory: cd python pip install --user pre-commit pre-commit install Afterwards, pre-commit will run whenever you commit. To run formatting and code-style separately, you can configure your IDE, such as VSCode, to use black and flake8, or run them via the command line: cd python flake8 hsml black hsml","title":"Python development setup"},{"location":"CONTRIBUTING/#python-documentation","text":"We follow a few best practices for writing the Python documentation: Use the google docstring style: \"\"\"[One Line Summary] [Extended Summary] [!!! example import xyz ] # Arguments arg1: Type[, optional]. Description[, defaults to `default`] arg2: Type[, optional]. Description[, defaults to `default`] # Returns Type. Description. # Raises Exception. Description. \"\"\" If Python 3 type annotations are used, they are inserted automatically. Model registry entity engine methods (e.g. ModelEngine etc.) only require a single line docstring. REST Api implementations (e.g. ModelApi etc.) should be fully documented with docstrings without defaults. Public Api such as metadata objects should be fully documented with defaults.","title":"Python documentation"},{"location":"CONTRIBUTING/#setup-and-build-documentation","text":"We use mkdocs together with mike ( for versioning ) to build the documentation and a plugin called keras-autodoc to auto generate Python API documentation from docstrings. Background about mike : mike builds the documentation and commits it as a new directory to the gh-pages branch. Each directory corresponds to one version of the documentation. Additionally, mike maintains a json in the root of gh-pages with the mappings of versions/aliases for each of the directories available. With aliases you can define extra names like dev or latest , to indicate stable and unstable releases. Currently we are using our own version of keras-autodoc pip install git+https://github.com/logicalclocks/keras-autodoc@split-tags-properties Install HSML with docs extras: pip install -e . [ dev,docs ] To build the docs, first run the auto doc script: cd .. python auto_doc.py","title":"Setup and Build Documentation"},{"location":"CONTRIBUTING/#option-1-build-only-current-version-of-docs","text":"Either build the docs, or serve them dynamically: Note: Links and pictures might not resolve properly later on when checking with this build. The reason for that is that the docs are deployed with versioning on docs.hopsworks.ai and therefore another level is added to all paths, e.g. docs.hopsworks.ai/[version-or-alias] . Using relative links should not be affected by this, however, building the docs with version (Option 2) is recommended. mkdocs build # or mkdocs serve","title":"Option 1: Build only current version of docs"},{"location":"CONTRIBUTING/#option-2-preferred-build-multi-version-doc-with-mike","text":"","title":"Option 2 (Preferred): Build multi-version doc with mike"},{"location":"CONTRIBUTING/#versioning-on-docshopsworksai","text":"On docs.hopsworks.ai we implement the following versioning scheme: current master branches (e.g. of hsml corresponding to master of Hopsworks): rendered as current Hopsworks snapshot version, e.g. 2.2.0-SNAPSHOT [dev] , where dev is an alias to indicate that this is an unstable version. the latest release: rendered with full current version, e.g. 2.1.5 [latest] with latest alias to indicate that this is the latest stable release. previous stable releases: rendered without alias, e.g. 2.1.4 .","title":"Versioning on docs.hopsworks.ai"},{"location":"CONTRIBUTING/#build-instructions","text":"For this you can either checkout and make a local copy of the upstream/gh-pages branch, where mike maintains the current state of docs.hopsworks.ai, or just build documentation for the branch you are updating: Building one branch: Checkout your dev branch with modified docs: git checkout [ dev-branch ] Generate API docs if necessary: python auto_doc.py Build docs with a version and alias mike deploy [ version ] [ alias ] --update-alias # for example, if you are updating documentation to be merged to master, # which will become the new SNAPSHOT version: mike deploy 2 .2.0-SNAPSHOT dev --update-alias # if you are updating docs of the latest stable release branch mike deploy [ version ] latest --update-alias # if you are updating docs of a previous stable release branch mike deploy [ version ] If no gh-pages branch existed in your local repository, this will have created it. Important : If no previous docs were built, you will have to choose a version as default to be loaded as index, as follows mike set-default [ version-or-alias ] You can now checkout the gh-pages branch and serve: git checkout gh-pages mike serve You can also list all available versions/aliases: mike list Delete and reset your local gh-pages branch: mike delete --all # or delete single version mike delete [ version-or-alias ]","title":"Build Instructions"},{"location":"CONTRIBUTING/#adding-new-api-documentation","text":"To add new documentation for APIs, you need to add information about the method/class to document to the auto_doc.py script: PAGES = { \"connection.md\" : [ \"hsml.connection.Connection.connection\" , \"hsml.connection.Connection.setup_databricks\" , ] \"new_template.md\" : [ \"module\" , \"xyz.asd\" ] } Now you can add a template markdown file to the docs/templates directory with the name you specified in the auto-doc script. The new_template.md file should contain a tag to identify the place at which the API documentation should be inserted: ## The XYZ package {{module}} Some extra content here. !!! example ```python import xyz ``` {{xyz.asd}} Finally, run the auto_doc.py script, as decribed above, to update the documentation. For information about Markdown syntax and possible Admonitions/Highlighting etc. see the Material for Mkdocs themes reference documentation .","title":"Adding new API documentation"},{"location":"generated/model-registry/connection_api/","text":"Connection # [source] Connection # hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project. Properties # [source] api_key_file # [source] api_key_value # [source] host # [source] hostname_verification # [source] port # [source] project # Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source] connection # Connection . connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsml.connection() . [source] get_model_registry # Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"Connection"},{"location":"generated/model-registry/connection_api/#connection","text":"[source]","title":"Connection"},{"location":"generated/model-registry/connection_api/#connection_1","text":"hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/model-registry/connection_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-registry/connection_api/#api_key_file","text":"[source]","title":"api_key_file"},{"location":"generated/model-registry/connection_api/#api_key_value","text":"[source]","title":"api_key_value"},{"location":"generated/model-registry/connection_api/#host","text":"[source]","title":"host"},{"location":"generated/model-registry/connection_api/#hostname_verification","text":"[source]","title":"hostname_verification"},{"location":"generated/model-registry/connection_api/#port","text":"[source]","title":"port"},{"location":"generated/model-registry/connection_api/#project","text":"","title":"project"},{"location":"generated/model-registry/connection_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/connection_api/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/model-registry/connection_api/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/model-registry/connection_api/#connection_2","text":"Connection . connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsml.connection() . [source]","title":"connection"},{"location":"generated/model-registry/connection_api/#get_model_registry","text":"Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source]","title":"get_model_registry"},{"location":"generated/model-registry/connection_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-registry/model/","text":"Model # A Model is an artifact that has been trained to recognize certain patterns. In addition to the artifact itself, a great deal of information is needed to describe it. The Model abstraction lets you save the model artifact along with metadata. Versioning # Models can be versioned. When a new Model is published to the Model Registry it is automatically incremented. Creation # [source] create_model # hsml . model_registry . ModelRegistry . tensorflow . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a TensorFlow model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. For more frameworks see Model API Retrieval # [source] get_model # ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. Properties # [source] created # Creation date of the model. [source] description # Description of the model. [source] environment # Input example of the model. [source] experiment_id # Experiment Id of the model. [source] experiment_project_name # experiment_project_name of the model. [source] framework # framework of the model. [source] id # Id of the model. [source] input_example # input_example of the model. [source] model_path # path of the model with version folder omitted. Resolves to /Projects/{project_name}/Models/{name} [source] model_registry_id # model_registry_id of the model. [source] model_schema # model schema of the model. [source] name # Name of the model. [source] program # Executable used to export the model. [source] project_name # project_name of the model. [source] shared_registry_project_name # shared_registry_project_name of the model. [source] training_dataset # training_dataset of the model. [source] training_metrics # Training metrics of the model. [source] user # user of the model. [source] version # Version of the model. [source] version_path # path of the model including version folder. Resolves to /Projects/{project_name}/Models/{name}/{version} Methods # [source] delete # Model . delete () Delete the model Potentially dangerous operation This operation drops all metadata associated with this version of the model and deletes the model files. Raises RestAPIError . [source] delete_tag # Model . delete_tag ( name ) Delete a tag attached to a model. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] download # Model . download () Download the model files to a local folder. [source] get_tag # Model . get_tag ( name ) Get the tags of a model. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # Model . get_tags () Retrieves all tags attached to a model. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] save # Model . save ( model_path , await_registration = 480 ) Persist this model including model files and metadata to the model registry. [source] set_tag # Model . set_tag ( name , value ) Attach a tag to a model. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value Union[str, dict] : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"Model"},{"location":"generated/model-registry/model/#model","text":"A Model is an artifact that has been trained to recognize certain patterns. In addition to the artifact itself, a great deal of information is needed to describe it. The Model abstraction lets you save the model artifact along with metadata.","title":"Model"},{"location":"generated/model-registry/model/#versioning","text":"Models can be versioned. When a new Model is published to the Model Registry it is automatically incremented.","title":"Versioning"},{"location":"generated/model-registry/model/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-registry/model/#create_model","text":"hsml . model_registry . ModelRegistry . tensorflow . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a TensorFlow model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. For more frameworks see Model API","title":"create_model"},{"location":"generated/model-registry/model/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-registry/model/#get_model","text":"ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry.","title":"get_model"},{"location":"generated/model-registry/model/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-registry/model/#created","text":"Creation date of the model. [source]","title":"created"},{"location":"generated/model-registry/model/#description","text":"Description of the model. [source]","title":"description"},{"location":"generated/model-registry/model/#environment","text":"Input example of the model. [source]","title":"environment"},{"location":"generated/model-registry/model/#experiment_id","text":"Experiment Id of the model. [source]","title":"experiment_id"},{"location":"generated/model-registry/model/#experiment_project_name","text":"experiment_project_name of the model. [source]","title":"experiment_project_name"},{"location":"generated/model-registry/model/#framework","text":"framework of the model. [source]","title":"framework"},{"location":"generated/model-registry/model/#id","text":"Id of the model. [source]","title":"id"},{"location":"generated/model-registry/model/#input_example","text":"input_example of the model. [source]","title":"input_example"},{"location":"generated/model-registry/model/#model_path","text":"path of the model with version folder omitted. Resolves to /Projects/{project_name}/Models/{name} [source]","title":"model_path"},{"location":"generated/model-registry/model/#model_registry_id","text":"model_registry_id of the model. [source]","title":"model_registry_id"},{"location":"generated/model-registry/model/#model_schema","text":"model schema of the model. [source]","title":"model_schema"},{"location":"generated/model-registry/model/#name","text":"Name of the model. [source]","title":"name"},{"location":"generated/model-registry/model/#program","text":"Executable used to export the model. [source]","title":"program"},{"location":"generated/model-registry/model/#project_name","text":"project_name of the model. [source]","title":"project_name"},{"location":"generated/model-registry/model/#shared_registry_project_name","text":"shared_registry_project_name of the model. [source]","title":"shared_registry_project_name"},{"location":"generated/model-registry/model/#training_dataset","text":"training_dataset of the model. [source]","title":"training_dataset"},{"location":"generated/model-registry/model/#training_metrics","text":"Training metrics of the model. [source]","title":"training_metrics"},{"location":"generated/model-registry/model/#user","text":"user of the model. [source]","title":"user"},{"location":"generated/model-registry/model/#version","text":"Version of the model. [source]","title":"version"},{"location":"generated/model-registry/model/#version_path","text":"path of the model including version folder. Resolves to /Projects/{project_name}/Models/{name}/{version}","title":"version_path"},{"location":"generated/model-registry/model/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/model/#delete","text":"Model . delete () Delete the model Potentially dangerous operation This operation drops all metadata associated with this version of the model and deletes the model files. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/model-registry/model/#delete_tag","text":"Model . delete_tag ( name ) Delete a tag attached to a model. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/model-registry/model/#download","text":"Model . download () Download the model files to a local folder. [source]","title":"download"},{"location":"generated/model-registry/model/#get_tag","text":"Model . get_tag ( name ) Get the tags of a model. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/model-registry/model/#get_tags","text":"Model . get_tags () Retrieves all tags attached to a model. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/model-registry/model/#save","text":"Model . save ( model_path , await_registration = 480 ) Persist this model including model files and metadata to the model registry. [source]","title":"save"},{"location":"generated/model-registry/model/#set_tag","text":"Model . set_tag ( name , value ) Attach a tag to a model. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value Union[str, dict] : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"set_tag"},{"location":"generated/model-registry/model_api/","text":"Model # Creation of a TensorFlow model # [source] create_model # hsml . model_registry . ModelRegistry . tensorflow . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a TensorFlow model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. Creation of a Torch model # [source] create_model # hsml . model_registry . ModelRegistry . torch . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a Torch model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. Creation of a scikit-learn model # [source] create_model # hsml . model_registry . ModelRegistry . sklearn . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create an SkLearn model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. Creation of a generic model # [source] create_model # hsml . model_registry . ModelRegistry . python . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a generic Python model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object. Retrieval # [source] get_model # ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. Properties # [source] created # Creation date of the model. [source] description # Description of the model. [source] environment # Input example of the model. [source] experiment_id # Experiment Id of the model. [source] experiment_project_name # experiment_project_name of the model. [source] framework # framework of the model. [source] id # Id of the model. [source] input_example # input_example of the model. [source] model_path # path of the model with version folder omitted. Resolves to /Projects/{project_name}/Models/{name} [source] model_registry_id # model_registry_id of the model. [source] model_schema # model schema of the model. [source] name # Name of the model. [source] program # Executable used to export the model. [source] project_name # project_name of the model. [source] shared_registry_project_name # shared_registry_project_name of the model. [source] training_dataset # training_dataset of the model. [source] training_metrics # Training metrics of the model. [source] user # user of the model. [source] version # Version of the model. [source] version_path # path of the model including version folder. Resolves to /Projects/{project_name}/Models/{name}/{version} Methods # [source] delete # Model . delete () Delete the model Potentially dangerous operation This operation drops all metadata associated with this version of the model and deletes the model files. Raises RestAPIError . [source] delete_tag # Model . delete_tag ( name ) Delete a tag attached to a model. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source] download # Model . download () Download the model files to a local folder. [source] get_tag # Model . get_tag ( name ) Get the tags of a model. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source] get_tags # Model . get_tags () Retrieves all tags attached to a model. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source] save # Model . save ( model_path , await_registration = 480 ) Persist this model including model files and metadata to the model registry. [source] set_tag # Model . set_tag ( name , value ) Attach a tag to a model. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value Union[str, dict] : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"Model"},{"location":"generated/model-registry/model_api/#model","text":"","title":"Model"},{"location":"generated/model-registry/model_api/#creation-of-a-tensorflow-model","text":"[source]","title":"Creation of a TensorFlow model"},{"location":"generated/model-registry/model_api/#create_model","text":"hsml . model_registry . ModelRegistry . tensorflow . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a TensorFlow model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object.","title":"create_model"},{"location":"generated/model-registry/model_api/#creation-of-a-torch-model","text":"[source]","title":"Creation of a Torch model"},{"location":"generated/model-registry/model_api/#create_model_1","text":"hsml . model_registry . ModelRegistry . torch . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a Torch model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object.","title":"create_model"},{"location":"generated/model-registry/model_api/#creation-of-a-scikit-learn-model","text":"[source]","title":"Creation of a scikit-learn model"},{"location":"generated/model-registry/model_api/#create_model_2","text":"hsml . model_registry . ModelRegistry . sklearn . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create an SkLearn model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object.","title":"create_model"},{"location":"generated/model-registry/model_api/#creation-of-a-generic-model","text":"[source]","title":"Creation of a generic model"},{"location":"generated/model-registry/model_api/#create_model_3","text":"hsml . model_registry . ModelRegistry . python . create_model ( name , version = None , metrics = None , description = None , input_example = None , model_schema = None ) Create a generic Python model metadata object. Lazy This method is lazy and does not persist any metadata or uploads model artifacts in the model registry on its own. To save the model object and the model artifacts, call the save() method with a local file path to the directory containing the model artifacts. Arguments name str : Name of the model to create. version Optional[int] : Optionally version of the model to create, defaults to None and will create the model with incremented version from the last version in the model registry. description Optional[str] : Optionally a string describing the model, defaults to empty string \"\" . input_example Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, numpy.ndarray, list]] : Optionally an input example that represents inputs for the model, defaults to None . model_schema Optional[hsml.model_schema.ModelSchema] : Optionally a model schema for the model inputs and/or outputs. Returns Model . The model metadata object.","title":"create_model"},{"location":"generated/model-registry/model_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-registry/model_api/#get_model","text":"ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry.","title":"get_model"},{"location":"generated/model-registry/model_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-registry/model_api/#created","text":"Creation date of the model. [source]","title":"created"},{"location":"generated/model-registry/model_api/#description","text":"Description of the model. [source]","title":"description"},{"location":"generated/model-registry/model_api/#environment","text":"Input example of the model. [source]","title":"environment"},{"location":"generated/model-registry/model_api/#experiment_id","text":"Experiment Id of the model. [source]","title":"experiment_id"},{"location":"generated/model-registry/model_api/#experiment_project_name","text":"experiment_project_name of the model. [source]","title":"experiment_project_name"},{"location":"generated/model-registry/model_api/#framework","text":"framework of the model. [source]","title":"framework"},{"location":"generated/model-registry/model_api/#id","text":"Id of the model. [source]","title":"id"},{"location":"generated/model-registry/model_api/#input_example","text":"input_example of the model. [source]","title":"input_example"},{"location":"generated/model-registry/model_api/#model_path","text":"path of the model with version folder omitted. Resolves to /Projects/{project_name}/Models/{name} [source]","title":"model_path"},{"location":"generated/model-registry/model_api/#model_registry_id","text":"model_registry_id of the model. [source]","title":"model_registry_id"},{"location":"generated/model-registry/model_api/#model_schema","text":"model schema of the model. [source]","title":"model_schema"},{"location":"generated/model-registry/model_api/#name","text":"Name of the model. [source]","title":"name"},{"location":"generated/model-registry/model_api/#program","text":"Executable used to export the model. [source]","title":"program"},{"location":"generated/model-registry/model_api/#project_name","text":"project_name of the model. [source]","title":"project_name"},{"location":"generated/model-registry/model_api/#shared_registry_project_name","text":"shared_registry_project_name of the model. [source]","title":"shared_registry_project_name"},{"location":"generated/model-registry/model_api/#training_dataset","text":"training_dataset of the model. [source]","title":"training_dataset"},{"location":"generated/model-registry/model_api/#training_metrics","text":"Training metrics of the model. [source]","title":"training_metrics"},{"location":"generated/model-registry/model_api/#user","text":"user of the model. [source]","title":"user"},{"location":"generated/model-registry/model_api/#version","text":"Version of the model. [source]","title":"version"},{"location":"generated/model-registry/model_api/#version_path","text":"path of the model including version folder. Resolves to /Projects/{project_name}/Models/{name}/{version}","title":"version_path"},{"location":"generated/model-registry/model_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/model_api/#delete","text":"Model . delete () Delete the model Potentially dangerous operation This operation drops all metadata associated with this version of the model and deletes the model files. Raises RestAPIError . [source]","title":"delete"},{"location":"generated/model-registry/model_api/#delete_tag","text":"Model . delete_tag ( name ) Delete a tag attached to a model. Arguments name str : Name of the tag to be removed. Raises RestAPIError in case the backend fails to delete the tag. [source]","title":"delete_tag"},{"location":"generated/model-registry/model_api/#download","text":"Model . download () Download the model files to a local folder. [source]","title":"download"},{"location":"generated/model-registry/model_api/#get_tag","text":"Model . get_tag ( name ) Get the tags of a model. Arguments name str : Name of the tag to get. Returns tag value Raises RestAPIError in case the backend fails to retrieve the tag. [source]","title":"get_tag"},{"location":"generated/model-registry/model_api/#get_tags","text":"Model . get_tags () Retrieves all tags attached to a model. Returns Dict[str, obj] of tags. Raises RestAPIError in case the backend fails to retrieve the tags. [source]","title":"get_tags"},{"location":"generated/model-registry/model_api/#save","text":"Model . save ( model_path , await_registration = 480 ) Persist this model including model files and metadata to the model registry. [source]","title":"save"},{"location":"generated/model-registry/model_api/#set_tag","text":"Model . set_tag ( name , value ) Attach a tag to a model. A tag consists of a pair. Tag names are unique identifiers across the whole cluster. The value of a tag can be any valid json - primitives, arrays or json objects. Arguments name str : Name of the tag to be added. value Union[str, dict] : Value of the tag to be added. Raises RestAPIError in case the backend fails to add the tag.","title":"set_tag"},{"location":"generated/model-registry/model_registry/","text":"Model Registry # Retrieval # [source] get_model_registry # Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. Modules # [source] python # Module for exporting a generic Python model. [source] sklearn # Module for exporting a sklearn model. [source] tensorflow # Module for exporting a TensorFlow model. [source] torch # Module for exporting a torch model. Properties # [source] model_registry_id # Id of the model registry. [source] project_id # Id of the project the registry is connected to. [source] project_name # Name of the project the registry is connected to. [source] shared_registry_project_name # Name of the project the shared model registry originates from. Methods # [source] get_best_model # ModelRegistry . get_best_model ( name , metric , direction ) Get the best performing model entity from the model registry. Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'. Arguments name str : Name of the model to get. metric str : Name of the key in the training metrics field to compare. direction str : 'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest. Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source] get_model # ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source] get_models # ModelRegistry . get_models ( name ) Get all model entities from the model registry for a specified name. Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name. Arguments name str : Name of the model to get. Returns List[Model] : A list of model metadata objects. Raises RestAPIError : If unable to retrieve model versions from the model registry.","title":"Model Registry"},{"location":"generated/model-registry/model_registry/#model-registry","text":"","title":"Model Registry"},{"location":"generated/model-registry/model_registry/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-registry/model_registry/#get_model_registry","text":"Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on.","title":"get_model_registry"},{"location":"generated/model-registry/model_registry/#modules","text":"[source]","title":"Modules"},{"location":"generated/model-registry/model_registry/#python","text":"Module for exporting a generic Python model. [source]","title":"python"},{"location":"generated/model-registry/model_registry/#sklearn","text":"Module for exporting a sklearn model. [source]","title":"sklearn"},{"location":"generated/model-registry/model_registry/#tensorflow","text":"Module for exporting a TensorFlow model. [source]","title":"tensorflow"},{"location":"generated/model-registry/model_registry/#torch","text":"Module for exporting a torch model.","title":"torch"},{"location":"generated/model-registry/model_registry/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-registry/model_registry/#model_registry_id","text":"Id of the model registry. [source]","title":"model_registry_id"},{"location":"generated/model-registry/model_registry/#project_id","text":"Id of the project the registry is connected to. [source]","title":"project_id"},{"location":"generated/model-registry/model_registry/#project_name","text":"Name of the project the registry is connected to. [source]","title":"project_name"},{"location":"generated/model-registry/model_registry/#shared_registry_project_name","text":"Name of the project the shared model registry originates from.","title":"shared_registry_project_name"},{"location":"generated/model-registry/model_registry/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/model_registry/#get_best_model","text":"ModelRegistry . get_best_model ( name , metric , direction ) Get the best performing model entity from the model registry. Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'. Arguments name str : Name of the model to get. metric str : Name of the key in the training metrics field to compare. direction str : 'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest. Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source]","title":"get_best_model"},{"location":"generated/model-registry/model_registry/#get_model","text":"ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source]","title":"get_model"},{"location":"generated/model-registry/model_registry/#get_models","text":"ModelRegistry . get_models ( name ) Get all model entities from the model registry for a specified name. Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name. Arguments name str : Name of the model to get. Returns List[Model] : A list of model metadata objects. Raises RestAPIError : If unable to retrieve model versions from the model registry.","title":"get_models"},{"location":"generated/model-registry/model_registry_api/","text":"Model Registry # Retrieval # [source] get_model_registry # Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. Modules # [source] python # Module for exporting a generic Python model. [source] sklearn # Module for exporting a sklearn model. [source] tensorflow # Module for exporting a TensorFlow model. [source] torch # Module for exporting a torch model. Properties # [source] model_registry_id # Id of the model registry. [source] project_id # Id of the project the registry is connected to. [source] project_name # Name of the project the registry is connected to. [source] shared_registry_project_name # Name of the project the shared model registry originates from. Methods # [source] get_best_model # ModelRegistry . get_best_model ( name , metric , direction ) Get the best performing model entity from the model registry. Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'. Arguments name str : Name of the model to get. metric str : Name of the key in the training metrics field to compare. direction str : 'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest. Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source] get_model # ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source] get_models # ModelRegistry . get_models ( name ) Get all model entities from the model registry for a specified name. Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name. Arguments name str : Name of the model to get. Returns List[Model] : A list of model metadata objects. Raises RestAPIError : If unable to retrieve model versions from the model registry.","title":"Model Registry"},{"location":"generated/model-registry/model_registry_api/#model-registry","text":"","title":"Model Registry"},{"location":"generated/model-registry/model_registry_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-registry/model_registry_api/#get_model_registry","text":"Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on.","title":"get_model_registry"},{"location":"generated/model-registry/model_registry_api/#modules","text":"[source]","title":"Modules"},{"location":"generated/model-registry/model_registry_api/#python","text":"Module for exporting a generic Python model. [source]","title":"python"},{"location":"generated/model-registry/model_registry_api/#sklearn","text":"Module for exporting a sklearn model. [source]","title":"sklearn"},{"location":"generated/model-registry/model_registry_api/#tensorflow","text":"Module for exporting a TensorFlow model. [source]","title":"tensorflow"},{"location":"generated/model-registry/model_registry_api/#torch","text":"Module for exporting a torch model.","title":"torch"},{"location":"generated/model-registry/model_registry_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-registry/model_registry_api/#model_registry_id","text":"Id of the model registry. [source]","title":"model_registry_id"},{"location":"generated/model-registry/model_registry_api/#project_id","text":"Id of the project the registry is connected to. [source]","title":"project_id"},{"location":"generated/model-registry/model_registry_api/#project_name","text":"Name of the project the registry is connected to. [source]","title":"project_name"},{"location":"generated/model-registry/model_registry_api/#shared_registry_project_name","text":"Name of the project the shared model registry originates from.","title":"shared_registry_project_name"},{"location":"generated/model-registry/model_registry_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/model_registry_api/#get_best_model","text":"ModelRegistry . get_best_model ( name , metric , direction ) Get the best performing model entity from the model registry. Getting the best performing model from the Model Registry means specifying in addition to the name, also a metric name corresponding to one of the keys in the training_metrics dict of the model and a direction. For example to get the model version with the highest accuracy, specify metric='accuracy' and direction='max'. Arguments name str : Name of the model to get. metric str : Name of the key in the training metrics field to compare. direction str : 'max' to get the model entity with the highest value of the set metric, or 'min' for the lowest. Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source]","title":"get_best_model"},{"location":"generated/model-registry/model_registry_api/#get_model","text":"ModelRegistry . get_model ( name , version = None ) Get a model entity from the model registry. Getting a model from the Model Registry means getting its metadata handle so you can subsequently download the model directory. Arguments name str : Name of the model to get. version Optional[int] : Version of the model to retrieve, defaults to None and will return the version=1 . Returns Model : The model metadata object. Raises RestAPIError : If unable to retrieve model from the model registry. [source]","title":"get_model"},{"location":"generated/model-registry/model_registry_api/#get_models","text":"ModelRegistry . get_models ( name ) Get all model entities from the model registry for a specified name. Getting all models from the Model Registry for a given name returns a list of model entities, one for each version registered under the specified model name. Arguments name str : Name of the model to get. Returns List[Model] : A list of model metadata objects. Raises RestAPIError : If unable to retrieve model versions from the model registry.","title":"get_models"},{"location":"generated/model-registry/model_schema/","text":"Model schema # A Model schema describes the input and outputs for a model. It provides a functional description of the model which makes it simpler to get started working with it. For example if the model inputs a tensor, the model schema can define the shape and data type of the tensor. A Model schema is composed of two Schema objects, one to describe the inputs and one to describe the outputs. It is not mandatory to define both the input and output schema when creating the Model schema. There are two different types of schemas, a column-based or tensor-based schema. Columnar schema # A column-based schema is composed of one or more columns, each column having a name if defined, and a data type. This maps directly to the schema you may find on for example a Spark or Pandas DataFrame. Build a schema # A column-based schema can be constructed manually by creating a list of dicts containing the mandatory type key which defines the data type. The optional keys name defines the column name, and description describe the field. In the following example, the model schema for a model trained on the Iris dataset is defined. # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs for iris dataset inputs = [{ 'name' : 'sepal_length' , 'type' : 'float' , 'description' : 'length of sepal leaves (cm)' }, { 'name' : 'sepal_width' , 'type' : 'float' , 'description' : 'width of sepal leaves (cm)' }, { 'name' : 'petal_length' , 'type' : 'float' , 'description' : 'length of petal leaves (cm)' }, { 'name' : 'petal_width' , 'type' : 'float' , 'description' : 'length of petal leaves (cm)' }] # Build the input schema input_schema = Schema ( inputs ) # Create ModelSchema object model_schema = ModelSchema ( input_schema = input_schema ) The created model schema can then be attached as metadata during creation of the Models object. import hsml conn = hsml . connection () mr = conn . get_model_registry () mnist_model = mr . python . create_model ( \"iris\" , model_schema = model_schema ) Infer a schema # A schema can also be inferred given a data object, such as for example a Pandas or Spark DataFrame. For a list of supported objects consult this . import pandas # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs defined in Pandas DataFrame data = [[ 5.1 , 3.5 , 1.4 , 0.2 ], [ 4.9 , 3.0 , 1.4 , 0.2 ]] pandas_df = pandas . DataFrame ( data , columns = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ]) # Infer the inputs schema input_schema = Schema ( pandas_df ) # Create ModelSchema object ModelSchema ( input_schema = input_schema ) Tensor schema # A tensor-based schema consists of one or more tensors, each tensor having a data type and a shape. This maps directly to the schema you may find on a numpy.ndarray . Build a schema # An inputs or outputs schema can be constructed manually by creating a list of dicts containing the mandatory key type defining the data type and shape defining the shape of the tensor. The optional key name defines the tensor name and description describe the field. In the following example, the model schema for a model trained on the MNIST dataset is defined. # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs for MNIST dataset inputs = [{ 'type' : 'uint8' , 'shape' : [ 28 , 28 , 1 ], 'description' : 'grayscale representation of 28x28 MNIST images' }] # Build the input schema input_schema = Schema ( inputs ) # Model outputs outputs = [{ 'type' : 'float32' , 'shape' : [ 10 ]}] # Build the output schema output_schema = Schema ( outputs ) # Create ModelSchema object model_schema = ModelSchema ( input_schema = input_schema , output_schema = output_schema ) The created model schema can then be attached as metadata during creation of the Models object. import hsml conn = hsml . connection () mr = conn . get_model_registry () mnist_model = mr . tensorflow . create_model ( \"mnist\" , model_schema = model_schema ) Infer a schema # A Tensor schema can also be inferred given a data object, currently we support numpy.ndarray . For a list of supported objects consult this . import numpy # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs defined in numpy.ndarray ndarr = numpy . random . rand ( 28 , 28 , 1 ) . astype ( \"uint8\" ) # Infer the inputs schema input_schema = Schema ( ndarr ) # Create ModelSchema object ModelSchema ( input_schema = input_schema )","title":"Model Schema"},{"location":"generated/model-registry/model_schema/#model-schema","text":"A Model schema describes the input and outputs for a model. It provides a functional description of the model which makes it simpler to get started working with it. For example if the model inputs a tensor, the model schema can define the shape and data type of the tensor. A Model schema is composed of two Schema objects, one to describe the inputs and one to describe the outputs. It is not mandatory to define both the input and output schema when creating the Model schema. There are two different types of schemas, a column-based or tensor-based schema.","title":"Model schema"},{"location":"generated/model-registry/model_schema/#columnar-schema","text":"A column-based schema is composed of one or more columns, each column having a name if defined, and a data type. This maps directly to the schema you may find on for example a Spark or Pandas DataFrame.","title":"Columnar schema"},{"location":"generated/model-registry/model_schema/#build-a-schema","text":"A column-based schema can be constructed manually by creating a list of dicts containing the mandatory type key which defines the data type. The optional keys name defines the column name, and description describe the field. In the following example, the model schema for a model trained on the Iris dataset is defined. # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs for iris dataset inputs = [{ 'name' : 'sepal_length' , 'type' : 'float' , 'description' : 'length of sepal leaves (cm)' }, { 'name' : 'sepal_width' , 'type' : 'float' , 'description' : 'width of sepal leaves (cm)' }, { 'name' : 'petal_length' , 'type' : 'float' , 'description' : 'length of petal leaves (cm)' }, { 'name' : 'petal_width' , 'type' : 'float' , 'description' : 'length of petal leaves (cm)' }] # Build the input schema input_schema = Schema ( inputs ) # Create ModelSchema object model_schema = ModelSchema ( input_schema = input_schema ) The created model schema can then be attached as metadata during creation of the Models object. import hsml conn = hsml . connection () mr = conn . get_model_registry () mnist_model = mr . python . create_model ( \"iris\" , model_schema = model_schema )","title":"Build a schema"},{"location":"generated/model-registry/model_schema/#infer-a-schema","text":"A schema can also be inferred given a data object, such as for example a Pandas or Spark DataFrame. For a list of supported objects consult this . import pandas # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs defined in Pandas DataFrame data = [[ 5.1 , 3.5 , 1.4 , 0.2 ], [ 4.9 , 3.0 , 1.4 , 0.2 ]] pandas_df = pandas . DataFrame ( data , columns = [ 'sepal_length' , 'sepal_width' , 'petal_length' , 'petal_width' ]) # Infer the inputs schema input_schema = Schema ( pandas_df ) # Create ModelSchema object ModelSchema ( input_schema = input_schema )","title":"Infer a schema"},{"location":"generated/model-registry/model_schema/#tensor-schema","text":"A tensor-based schema consists of one or more tensors, each tensor having a data type and a shape. This maps directly to the schema you may find on a numpy.ndarray .","title":"Tensor schema"},{"location":"generated/model-registry/model_schema/#build-a-schema_1","text":"An inputs or outputs schema can be constructed manually by creating a list of dicts containing the mandatory key type defining the data type and shape defining the shape of the tensor. The optional key name defines the tensor name and description describe the field. In the following example, the model schema for a model trained on the MNIST dataset is defined. # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs for MNIST dataset inputs = [{ 'type' : 'uint8' , 'shape' : [ 28 , 28 , 1 ], 'description' : 'grayscale representation of 28x28 MNIST images' }] # Build the input schema input_schema = Schema ( inputs ) # Model outputs outputs = [{ 'type' : 'float32' , 'shape' : [ 10 ]}] # Build the output schema output_schema = Schema ( outputs ) # Create ModelSchema object model_schema = ModelSchema ( input_schema = input_schema , output_schema = output_schema ) The created model schema can then be attached as metadata during creation of the Models object. import hsml conn = hsml . connection () mr = conn . get_model_registry () mnist_model = mr . tensorflow . create_model ( \"mnist\" , model_schema = model_schema )","title":"Build a schema"},{"location":"generated/model-registry/model_schema/#infer-a-schema_1","text":"A Tensor schema can also be inferred given a data object, currently we support numpy.ndarray . For a list of supported objects consult this . import numpy # Import a Schema and ModelSchema definition from hsml.utils.model_schema import ModelSchema from hsml.utils.schema import Schema # Model inputs defined in numpy.ndarray ndarr = numpy . random . rand ( 28 , 28 , 1 ) . astype ( \"uint8\" ) # Infer the inputs schema input_schema = Schema ( ndarr ) # Create ModelSchema object ModelSchema ( input_schema = input_schema )","title":"Infer a schema"},{"location":"generated/model-registry/model_schema_api/","text":"Model Schema # Creation # To create a ModelSchema, the schema of the Model inputs and/or Model ouputs has to be defined beforehand. [source] Schema # hsml . schema . Schema ( object = None ) Create a schema for a model input or output. Arguments object Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, pyspark.sql.dataframe.DataFrame, hsfs.training_dataset.TrainingDataset, numpy.ndarray, list]] : The object to construct the schema from. Returns Schema . The schema object. After defining the Model inputs and/or outputs schemas, a ModelSchema can be created using its class constructor. [source] ModelSchema # hsml . model_schema . ModelSchema ( input_schema = None , output_schema = None ) Create a schema for a model. Arguments input_schema Optional[hsml.schema.Schema] : Schema to describe the inputs. output_schema Optional[hsml.schema.Schema] : Schema to describe the outputs. Returns ModelSchema . The model schema object. Retrieval # Model Schema # Model schemas can be accessed from the model metadata objects. model . model_schema Model Input & Ouput Schemas # The schemas of the Model inputs and outputs can be accessed from the ModelSchema metadata objects. model_schema . input_schema model_schema . output_schema Methods # [source] to_dict # Schema . to_dict () Get dict representation of the Schema. [source] to_dict # ModelSchema . to_dict () Get dict representation of the ModelSchema.","title":"Model Schema"},{"location":"generated/model-registry/model_schema_api/#model-schema","text":"","title":"Model Schema"},{"location":"generated/model-registry/model_schema_api/#creation","text":"To create a ModelSchema, the schema of the Model inputs and/or Model ouputs has to be defined beforehand. [source]","title":"Creation"},{"location":"generated/model-registry/model_schema_api/#schema","text":"hsml . schema . Schema ( object = None ) Create a schema for a model input or output. Arguments object Optional[Union[pandas.core.frame.DataFrame, pandas.core.series.Series, pyspark.sql.dataframe.DataFrame, hsfs.training_dataset.TrainingDataset, numpy.ndarray, list]] : The object to construct the schema from. Returns Schema . The schema object. After defining the Model inputs and/or outputs schemas, a ModelSchema can be created using its class constructor. [source]","title":"Schema"},{"location":"generated/model-registry/model_schema_api/#modelschema","text":"hsml . model_schema . ModelSchema ( input_schema = None , output_schema = None ) Create a schema for a model. Arguments input_schema Optional[hsml.schema.Schema] : Schema to describe the inputs. output_schema Optional[hsml.schema.Schema] : Schema to describe the outputs. Returns ModelSchema . The model schema object.","title":"ModelSchema"},{"location":"generated/model-registry/model_schema_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-registry/model_schema_api/#model-schema_1","text":"Model schemas can be accessed from the model metadata objects. model . model_schema","title":"Model Schema"},{"location":"generated/model-registry/model_schema_api/#model-input-ouput-schemas","text":"The schemas of the Model inputs and outputs can be accessed from the ModelSchema metadata objects. model_schema . input_schema model_schema . output_schema","title":"Model Input &amp; Ouput Schemas"},{"location":"generated/model-registry/model_schema_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/model_schema_api/#to_dict","text":"Schema . to_dict () Get dict representation of the Schema. [source]","title":"to_dict"},{"location":"generated/model-registry/model_schema_api/#to_dict_1","text":"ModelSchema . to_dict () Get dict representation of the ModelSchema.","title":"to_dict"},{"location":"generated/model-registry/project/","text":"Project/Connection # In Hopsworks a Project is a sandboxed collection of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Model Registry and Model Serving. However, it is possible to share a Model Registry and a Model Serving among projects. When working with the Model Management (Registry and Serving) from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Model Registries and Model Servings simultaneously. The connection object to a Hopsworks instance is represented by a Connection object . The handle can then be used to retrieve a reference to the Model Management ( Registry and Serving ) you want to operate on. Examples # Python Connecting from Hopsworks import hsml conn = hsml . connection () Connecting from Python environment To connect from an external Python environment you have to provide the api key, project name, and hostname for your Hopsworks cluster. Here, we pass the api key with the insecure api_key_value parameter: import hsml conn = hsml . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_ml_admin000\" , hostname_verification = False , api_key_value = \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" ) A more secure approach for passing the API key is as a file: import hsml conn = hsml . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_ml_admin000\" , hostname_verification = False , api_key_file = \"modelregistry.key\" ) Getting the Model Management (Registry and Serving) handle mr = conn . get_model_registry () ms = conn . get_model_serving () Sharing a Model Registry # Connections are on a project-level, however, it is possible to share model registries among projects, so even if you have a connection to one project, you can retrieve a handle to any model registry shared with that project. To share a model registry, you can follow these steps: Sharing a Model Registry Open the project of the model registry that you would like to share on Hopsworks. Go to the Data Set browser and right click the Models entry. Click Share with , then select Project and choose the project you wish to share the model registry with. Select the permissions level that the project user members should have on the model registry and click Share . Open the project you just shared the model registry with. Go to the Data Sets browser and there you should see the shared model registry as [project_name_of_shared_model_registry]::Models . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this model registry from the other project. Sharing a model registry between projects Accepting a shared model registry from a project Connection Handle # [source] Connection # hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project. Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source] get_model_registry # Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"Project/Connection"},{"location":"generated/model-registry/project/#projectconnection","text":"In Hopsworks a Project is a sandboxed collection of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Model Registry and Model Serving. However, it is possible to share a Model Registry and a Model Serving among projects. When working with the Model Management (Registry and Serving) from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Model Registries and Model Servings simultaneously. The connection object to a Hopsworks instance is represented by a Connection object . The handle can then be used to retrieve a reference to the Model Management ( Registry and Serving ) you want to operate on.","title":"Project/Connection"},{"location":"generated/model-registry/project/#examples","text":"Python Connecting from Hopsworks import hsml conn = hsml . connection () Connecting from Python environment To connect from an external Python environment you have to provide the api key, project name, and hostname for your Hopsworks cluster. Here, we pass the api key with the insecure api_key_value parameter: import hsml conn = hsml . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_ml_admin000\" , hostname_verification = False , api_key_value = \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" ) A more secure approach for passing the API key is as a file: import hsml conn = hsml . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_ml_admin000\" , hostname_verification = False , api_key_file = \"modelregistry.key\" ) Getting the Model Management (Registry and Serving) handle mr = conn . get_model_registry () ms = conn . get_model_serving ()","title":"Examples"},{"location":"generated/model-registry/project/#sharing-a-model-registry","text":"Connections are on a project-level, however, it is possible to share model registries among projects, so even if you have a connection to one project, you can retrieve a handle to any model registry shared with that project. To share a model registry, you can follow these steps: Sharing a Model Registry Open the project of the model registry that you would like to share on Hopsworks. Go to the Data Set browser and right click the Models entry. Click Share with , then select Project and choose the project you wish to share the model registry with. Select the permissions level that the project user members should have on the model registry and click Share . Open the project you just shared the model registry with. Go to the Data Sets browser and there you should see the shared model registry as [project_name_of_shared_model_registry]::Models . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this model registry from the other project. Sharing a model registry between projects Accepting a shared model registry from a project","title":"Sharing a Model Registry"},{"location":"generated/model-registry/project/#connection-handle","text":"[source]","title":"Connection Handle"},{"location":"generated/model-registry/project/#connection","text":"hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/model-registry/project/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-registry/project/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/model-registry/project/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/model-registry/project/#get_model_registry","text":"Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source]","title":"get_model_registry"},{"location":"generated/model-registry/project/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/connection_api/","text":"Connection # [source] Connection # hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project. Properties # [source] api_key_file # [source] api_key_value # [source] host # [source] hostname_verification # [source] port # [source] project # Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source] connection # Connection . connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsml.connection() . [source] get_model_registry # Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"Connection"},{"location":"generated/model-serving/connection_api/#connection","text":"[source]","title":"Connection"},{"location":"generated/model-serving/connection_api/#connection_1","text":"hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/model-serving/connection_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/connection_api/#api_key_file","text":"[source]","title":"api_key_file"},{"location":"generated/model-serving/connection_api/#api_key_value","text":"[source]","title":"api_key_value"},{"location":"generated/model-serving/connection_api/#host","text":"[source]","title":"host"},{"location":"generated/model-serving/connection_api/#hostname_verification","text":"[source]","title":"hostname_verification"},{"location":"generated/model-serving/connection_api/#port","text":"[source]","title":"port"},{"location":"generated/model-serving/connection_api/#project","text":"","title":"project"},{"location":"generated/model-serving/connection_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/connection_api/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/model-serving/connection_api/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/model-serving/connection_api/#connection_2","text":"Connection . connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) Connection factory method, accessible through hsml.connection() . [source]","title":"connection"},{"location":"generated/model-serving/connection_api/#get_model_registry","text":"Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source]","title":"get_model_registry"},{"location":"generated/model-serving/connection_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/deployment/","text":"Deployment # Deployments are used to unify the different components involved in making one or more trained models online and accessible for the computation of predictions on demand. In each deployment, there are three main components to consider: Model artifacts Predictors Transformers See how to start creating deployments in the Model Serving Quickstart . Components # Model artifact # A model artifact is a package containing all of the necessary files for the deployment of a model. It includes the model file(s) and/or custom scripts for loading the model (predictor script) or transforming the model inputs at inference time (the transformer script). When a new deployment is created, a model artifact is generated in two cases: the artifact version in the predictor is set to CREATE (see Artifact Version ) no model artifact with the same files has been created before. Info Model artifacts are assigned an incremental version number, being 0 the version reserved for model artifacts that do not contain predictor or transformer scripts (i.e., shared artifacts containing only the model files). Predictor # Predictors are responsible for running the model server that loads the trained model, listens to inference requests and returns prediction results. To learn more about predictors, see the Predictor Guide Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon. Transformer # Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Info Transformers are only supported in KServe deployments. For more details, see the Predictor Guide . Properties # [source] artifact_version # Artifact version deployed by the predictor. [source] created_at # Created at date of the predictor. [source] creator # Creator of the predictor. [source] id # Id of the deployment. [source] inference_batcher # Configuration of the inference batcher attached to this predictor. [source] inference_logger # Configuration of the inference logger attached to this predictor. [source] model_name # Name of the model deployed by the predictor [source] model_path # Model path deployed by the predictor. [source] model_server # Model server ran by the predictor. [source] model_version # Model version deployed by the predictor. [source] name # Name of the deployment. [source] predictor # Predictor used in the deployment. [source] requested_instances # Total number of requested instances in the deployment. [source] resources # Resource configuration for the predictor. [source] script_file # Script file used by the predictor. [source] serving_tool # Serving tool used to run the model server. [source] transformer # Transformer configured in the predictor. Methods # [source] delete # Deployment . delete () Delete the deployment [source] describe # Deployment . describe () Print a description of the deployment [source] get_state # Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment. [source] predict # Deployment . predict ( data ) Send inference requests to the deployment Arguments data dict : Payload of the inference request. Returns dict . Inference response. [source] save # Deployment . save () Persist this deployment including the predictor and metadata to Model Serving. [source] start # Deployment . start ( await_running = 60 ) Start the deployment Arguments await_running Optional[int] : Awaiting time (seconds) for the deployment to start. If the deployment has not started within this timespan, the call to this method returns while it deploys in the background. [source] stop # Deployment . stop ( await_stopped = 60 ) Stop the deployment Arguments await_stopped Optional[int] : Awaiting time (seconds) for the deployment to stop. If the deployment has not stopped within this timespan, the call to this method returns while it stopping in the background. [source] to_dict # Deployment . to_dict ()","title":"Deployment"},{"location":"generated/model-serving/deployment/#deployment","text":"Deployments are used to unify the different components involved in making one or more trained models online and accessible for the computation of predictions on demand. In each deployment, there are three main components to consider: Model artifacts Predictors Transformers See how to start creating deployments in the Model Serving Quickstart .","title":"Deployment"},{"location":"generated/model-serving/deployment/#components","text":"","title":"Components"},{"location":"generated/model-serving/deployment/#model-artifact","text":"A model artifact is a package containing all of the necessary files for the deployment of a model. It includes the model file(s) and/or custom scripts for loading the model (predictor script) or transforming the model inputs at inference time (the transformer script). When a new deployment is created, a model artifact is generated in two cases: the artifact version in the predictor is set to CREATE (see Artifact Version ) no model artifact with the same files has been created before. Info Model artifacts are assigned an incremental version number, being 0 the version reserved for model artifacts that do not contain predictor or transformer scripts (i.e., shared artifacts containing only the model files).","title":"Model artifact"},{"location":"generated/model-serving/deployment/#predictor","text":"Predictors are responsible for running the model server that loads the trained model, listens to inference requests and returns prediction results. To learn more about predictors, see the Predictor Guide Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon.","title":"Predictor"},{"location":"generated/model-serving/deployment/#transformer","text":"Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. To learn more about transformers, see the Transformer Guide . Info Transformers are only supported in KServe deployments. For more details, see the Predictor Guide .","title":"Transformer"},{"location":"generated/model-serving/deployment/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/deployment/#artifact_version","text":"Artifact version deployed by the predictor. [source]","title":"artifact_version"},{"location":"generated/model-serving/deployment/#created_at","text":"Created at date of the predictor. [source]","title":"created_at"},{"location":"generated/model-serving/deployment/#creator","text":"Creator of the predictor. [source]","title":"creator"},{"location":"generated/model-serving/deployment/#id","text":"Id of the deployment. [source]","title":"id"},{"location":"generated/model-serving/deployment/#inference_batcher","text":"Configuration of the inference batcher attached to this predictor. [source]","title":"inference_batcher"},{"location":"generated/model-serving/deployment/#inference_logger","text":"Configuration of the inference logger attached to this predictor. [source]","title":"inference_logger"},{"location":"generated/model-serving/deployment/#model_name","text":"Name of the model deployed by the predictor [source]","title":"model_name"},{"location":"generated/model-serving/deployment/#model_path","text":"Model path deployed by the predictor. [source]","title":"model_path"},{"location":"generated/model-serving/deployment/#model_server","text":"Model server ran by the predictor. [source]","title":"model_server"},{"location":"generated/model-serving/deployment/#model_version","text":"Model version deployed by the predictor. [source]","title":"model_version"},{"location":"generated/model-serving/deployment/#name","text":"Name of the deployment. [source]","title":"name"},{"location":"generated/model-serving/deployment/#predictor_1","text":"Predictor used in the deployment. [source]","title":"predictor"},{"location":"generated/model-serving/deployment/#requested_instances","text":"Total number of requested instances in the deployment. [source]","title":"requested_instances"},{"location":"generated/model-serving/deployment/#resources","text":"Resource configuration for the predictor. [source]","title":"resources"},{"location":"generated/model-serving/deployment/#script_file","text":"Script file used by the predictor. [source]","title":"script_file"},{"location":"generated/model-serving/deployment/#serving_tool","text":"Serving tool used to run the model server. [source]","title":"serving_tool"},{"location":"generated/model-serving/deployment/#transformer_1","text":"Transformer configured in the predictor.","title":"transformer"},{"location":"generated/model-serving/deployment/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/deployment/#delete","text":"Deployment . delete () Delete the deployment [source]","title":"delete"},{"location":"generated/model-serving/deployment/#describe","text":"Deployment . describe () Print a description of the deployment [source]","title":"describe"},{"location":"generated/model-serving/deployment/#get_state","text":"Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment. [source]","title":"get_state"},{"location":"generated/model-serving/deployment/#predict","text":"Deployment . predict ( data ) Send inference requests to the deployment Arguments data dict : Payload of the inference request. Returns dict . Inference response. [source]","title":"predict"},{"location":"generated/model-serving/deployment/#save","text":"Deployment . save () Persist this deployment including the predictor and metadata to Model Serving. [source]","title":"save"},{"location":"generated/model-serving/deployment/#start","text":"Deployment . start ( await_running = 60 ) Start the deployment Arguments await_running Optional[int] : Awaiting time (seconds) for the deployment to start. If the deployment has not started within this timespan, the call to this method returns while it deploys in the background. [source]","title":"start"},{"location":"generated/model-serving/deployment/#stop","text":"Deployment . stop ( await_stopped = 60 ) Stop the deployment Arguments await_stopped Optional[int] : Awaiting time (seconds) for the deployment to stop. If the deployment has not stopped within this timespan, the call to this method returns while it stopping in the background. [source]","title":"stop"},{"location":"generated/model-serving/deployment/#to_dict","text":"Deployment . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/deployment_api/","text":"Deployment # Handle # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on. Creation # [source] create_deployment # ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source] deploy # Model . deploy ( name = None , artifact_version = \"CREATE\" , model_server = None , serving_tool = None , script_file = None , resources = {}, inference_logger = {}, inference_batcher = None , transformer = None , ) Deploy the model. Arguments name Optional[str] : Name of the deployment. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. model_server Optional[str] : Model server ran by the predictor. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Deployment . The deployment metadata object. [source] deploy # Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Returns Deployment . The deployment metadata object. Retrieval # [source] get_deployment # ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployment_by_id # ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployments # ModelServing . get_deployments () Get all deployments from model serving. Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving. Properties # [source] artifact_version # Artifact version deployed by the predictor. [source] created_at # Created at date of the predictor. [source] creator # Creator of the predictor. [source] id # Id of the deployment. [source] inference_batcher # Configuration of the inference batcher attached to this predictor. [source] inference_logger # Configuration of the inference logger attached to this predictor. [source] model_name # Name of the model deployed by the predictor [source] model_path # Model path deployed by the predictor. [source] model_server # Model server ran by the predictor. [source] model_version # Model version deployed by the predictor. [source] name # Name of the deployment. [source] predictor # Predictor used in the deployment. [source] requested_instances # Total number of requested instances in the deployment. [source] resources # Resource configuration for the predictor. [source] script_file # Script file used by the predictor. [source] serving_tool # Serving tool used to run the model server. [source] transformer # Transformer configured in the predictor. Methods # [source] delete # Deployment . delete () Delete the deployment [source] describe # Deployment . describe () Print a description of the deployment [source] get_state # Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment. [source] predict # Deployment . predict ( data ) Send inference requests to the deployment Arguments data dict : Payload of the inference request. Returns dict . Inference response. [source] save # Deployment . save () Persist this deployment including the predictor and metadata to Model Serving. [source] start # Deployment . start ( await_running = 60 ) Start the deployment Arguments await_running Optional[int] : Awaiting time (seconds) for the deployment to start. If the deployment has not started within this timespan, the call to this method returns while it deploys in the background. [source] stop # Deployment . stop ( await_stopped = 60 ) Stop the deployment Arguments await_stopped Optional[int] : Awaiting time (seconds) for the deployment to stop. If the deployment has not stopped within this timespan, the call to this method returns while it stopping in the background. [source] to_dict # Deployment . to_dict ()","title":"Deployment"},{"location":"generated/model-serving/deployment_api/#deployment","text":"","title":"Deployment"},{"location":"generated/model-serving/deployment_api/#handle","text":"[source]","title":"Handle"},{"location":"generated/model-serving/deployment_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/deployment_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/deployment_api/#create_deployment","text":"ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source]","title":"create_deployment"},{"location":"generated/model-serving/deployment_api/#deploy","text":"Model . deploy ( name = None , artifact_version = \"CREATE\" , model_server = None , serving_tool = None , script_file = None , resources = {}, inference_logger = {}, inference_batcher = None , transformer = None , ) Deploy the model. Arguments name Optional[str] : Name of the deployment. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. model_server Optional[str] : Model server ran by the predictor. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Deployment . The deployment metadata object. [source]","title":"deploy"},{"location":"generated/model-serving/deployment_api/#deploy_1","text":"Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Returns Deployment . The deployment metadata object.","title":"deploy"},{"location":"generated/model-serving/deployment_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-serving/deployment_api/#get_deployment","text":"ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment"},{"location":"generated/model-serving/deployment_api/#get_deployment_by_id","text":"ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment_by_id"},{"location":"generated/model-serving/deployment_api/#get_deployments","text":"ModelServing . get_deployments () Get all deployments from model serving. Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving.","title":"get_deployments"},{"location":"generated/model-serving/deployment_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/deployment_api/#artifact_version","text":"Artifact version deployed by the predictor. [source]","title":"artifact_version"},{"location":"generated/model-serving/deployment_api/#created_at","text":"Created at date of the predictor. [source]","title":"created_at"},{"location":"generated/model-serving/deployment_api/#creator","text":"Creator of the predictor. [source]","title":"creator"},{"location":"generated/model-serving/deployment_api/#id","text":"Id of the deployment. [source]","title":"id"},{"location":"generated/model-serving/deployment_api/#inference_batcher","text":"Configuration of the inference batcher attached to this predictor. [source]","title":"inference_batcher"},{"location":"generated/model-serving/deployment_api/#inference_logger","text":"Configuration of the inference logger attached to this predictor. [source]","title":"inference_logger"},{"location":"generated/model-serving/deployment_api/#model_name","text":"Name of the model deployed by the predictor [source]","title":"model_name"},{"location":"generated/model-serving/deployment_api/#model_path","text":"Model path deployed by the predictor. [source]","title":"model_path"},{"location":"generated/model-serving/deployment_api/#model_server","text":"Model server ran by the predictor. [source]","title":"model_server"},{"location":"generated/model-serving/deployment_api/#model_version","text":"Model version deployed by the predictor. [source]","title":"model_version"},{"location":"generated/model-serving/deployment_api/#name","text":"Name of the deployment. [source]","title":"name"},{"location":"generated/model-serving/deployment_api/#predictor","text":"Predictor used in the deployment. [source]","title":"predictor"},{"location":"generated/model-serving/deployment_api/#requested_instances","text":"Total number of requested instances in the deployment. [source]","title":"requested_instances"},{"location":"generated/model-serving/deployment_api/#resources","text":"Resource configuration for the predictor. [source]","title":"resources"},{"location":"generated/model-serving/deployment_api/#script_file","text":"Script file used by the predictor. [source]","title":"script_file"},{"location":"generated/model-serving/deployment_api/#serving_tool","text":"Serving tool used to run the model server. [source]","title":"serving_tool"},{"location":"generated/model-serving/deployment_api/#transformer","text":"Transformer configured in the predictor.","title":"transformer"},{"location":"generated/model-serving/deployment_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/deployment_api/#delete","text":"Deployment . delete () Delete the deployment [source]","title":"delete"},{"location":"generated/model-serving/deployment_api/#describe","text":"Deployment . describe () Print a description of the deployment [source]","title":"describe"},{"location":"generated/model-serving/deployment_api/#get_state","text":"Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment. [source]","title":"get_state"},{"location":"generated/model-serving/deployment_api/#predict","text":"Deployment . predict ( data ) Send inference requests to the deployment Arguments data dict : Payload of the inference request. Returns dict . Inference response. [source]","title":"predict"},{"location":"generated/model-serving/deployment_api/#save","text":"Deployment . save () Persist this deployment including the predictor and metadata to Model Serving. [source]","title":"save"},{"location":"generated/model-serving/deployment_api/#start","text":"Deployment . start ( await_running = 60 ) Start the deployment Arguments await_running Optional[int] : Awaiting time (seconds) for the deployment to start. If the deployment has not started within this timespan, the call to this method returns while it deploys in the background. [source]","title":"start"},{"location":"generated/model-serving/deployment_api/#stop","text":"Deployment . stop ( await_stopped = 60 ) Stop the deployment Arguments await_stopped Optional[int] : Awaiting time (seconds) for the deployment to stop. If the deployment has not stopped within this timespan, the call to this method returns while it stopping in the background. [source]","title":"stop"},{"location":"generated/model-serving/deployment_api/#to_dict","text":"Deployment . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/inference_batcher_api/","text":"Inference batcher # Creation # [source] InferenceBatcher # hsml . inference_batcher . InferenceBatcher ( enabled = None ) Configuration of an inference batcher for a predictor. Arguments enabled Optional[bool] : Whether the inference batcher is enabled or not. The default value is false . Returns InferenceLogger . Configuration of an inference logger. Retrieval # predictor.inference_batcher # Inference batchers can be accessed from the predictor metadata objects. predictor . inference_batcher Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference . Properties # [source] enabled # Whether the inference batcher is enabled or not. Methods # [source] describe # InferenceBatcher . describe () Print a description of the inference batcher [source] to_dict # InferenceBatcher . to_dict ()","title":"Inference Batcher"},{"location":"generated/model-serving/inference_batcher_api/#inference-batcher","text":"","title":"Inference batcher"},{"location":"generated/model-serving/inference_batcher_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/inference_batcher_api/#inferencebatcher","text":"hsml . inference_batcher . InferenceBatcher ( enabled = None ) Configuration of an inference batcher for a predictor. Arguments enabled Optional[bool] : Whether the inference batcher is enabled or not. The default value is false . Returns InferenceLogger . Configuration of an inference logger.","title":"InferenceBatcher"},{"location":"generated/model-serving/inference_batcher_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/inference_batcher_api/#predictorinference_batcher","text":"Inference batchers can be accessed from the predictor metadata objects. predictor . inference_batcher Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference .","title":"predictor.inference_batcher"},{"location":"generated/model-serving/inference_batcher_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/inference_batcher_api/#enabled","text":"Whether the inference batcher is enabled or not.","title":"enabled"},{"location":"generated/model-serving/inference_batcher_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/inference_batcher_api/#describe","text":"InferenceBatcher . describe () Print a description of the inference batcher [source]","title":"describe"},{"location":"generated/model-serving/inference_batcher_api/#to_dict","text":"InferenceBatcher . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/inference_logger_api/","text":"Inference logger # Creation # [source] InferenceLogger # hsml . inference_logger . InferenceLogger ( kafka_topic = {}, mode = \"ALL\" ) Configuration of an inference logger for a predictor. Arguments kafka_topic Optional[Union[hsml.kafka_topic.KafkaTopic, dict]] : Kafka topic to send the inference logs to. By default, a new Kafka topic is configured. mode Optional[str] : Inference logging mode. (e.g., NONE , ALL , PREDICTIONS , or MODEL_INPUTS ). By default, ALL inference logs are sent. Returns InferenceLogger . Configuration of an inference logger. Retrieval # predictor.inference_logger # Inference loggers can be accessed from the predictor metadata objects. predictor . inference_logger Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference . Properties # [source] kafka_topic # Kafka topic to send the inference logs to. [source] mode # Inference logging mode (\"NONE\", \"ALL\", \"PREDICTIONS\", or \"MODEL_INPUTS\"). Methods # [source] describe # InferenceLogger . describe () Print a description of the inference logger [source] to_dict # InferenceLogger . to_dict ()","title":"Inference Logger"},{"location":"generated/model-serving/inference_logger_api/#inference-logger","text":"","title":"Inference logger"},{"location":"generated/model-serving/inference_logger_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/inference_logger_api/#inferencelogger","text":"hsml . inference_logger . InferenceLogger ( kafka_topic = {}, mode = \"ALL\" ) Configuration of an inference logger for a predictor. Arguments kafka_topic Optional[Union[hsml.kafka_topic.KafkaTopic, dict]] : Kafka topic to send the inference logs to. By default, a new Kafka topic is configured. mode Optional[str] : Inference logging mode. (e.g., NONE , ALL , PREDICTIONS , or MODEL_INPUTS ). By default, ALL inference logs are sent. Returns InferenceLogger . Configuration of an inference logger.","title":"InferenceLogger"},{"location":"generated/model-serving/inference_logger_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/inference_logger_api/#predictorinference_logger","text":"Inference loggers can be accessed from the predictor metadata objects. predictor . inference_logger Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference .","title":"predictor.inference_logger"},{"location":"generated/model-serving/inference_logger_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/inference_logger_api/#kafka_topic","text":"Kafka topic to send the inference logs to. [source]","title":"kafka_topic"},{"location":"generated/model-serving/inference_logger_api/#mode","text":"Inference logging mode (\"NONE\", \"ALL\", \"PREDICTIONS\", or \"MODEL_INPUTS\").","title":"mode"},{"location":"generated/model-serving/inference_logger_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/inference_logger_api/#describe","text":"InferenceLogger . describe () Print a description of the inference logger [source]","title":"describe"},{"location":"generated/model-serving/inference_logger_api/#to_dict","text":"InferenceLogger . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/model_serving/","text":"Model Serving # The Model Serving handle acts as the starting point for creating new deployments and retrieving already existing ones. See how to start creating deployments in the Model Serving Quickstart . To operate on specific deployments, you can get the deployment by name from Hopsworks Model Serving using the ms.get_deployment() method and then use the metadata object to perform the desired actions on it. Python # Get deployment by name deployment = ms . get_deployment ( \"awesome-deployment\" ) To learn what kind of operations can be done on a deployment, see the Deployment Guide . Retrieval # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on. Properties # [source] project_id # Id of the project in which Model Serving is located. [source] project_name # Name of the project in which Model Serving is located. Methods # [source] create_deployment # ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source] create_predictor # ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , model_server = None , serving_tool = None , script_file = None , resources = {}, inference_logger = {}, inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. model_server Optional[str] : Model server ran by the predictor. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object. [source] create_transformer # ModelServing . create_transformer ( script_file = None , resources = {}) Create a Transformer metadata object. Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object. [source] get_deployment # ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployment_by_id # ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployments # ModelServing . get_deployments () Get all deployments from model serving. Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving.","title":"Model Serving"},{"location":"generated/model-serving/model_serving/#model-serving","text":"The Model Serving handle acts as the starting point for creating new deployments and retrieving already existing ones. See how to start creating deployments in the Model Serving Quickstart . To operate on specific deployments, you can get the deployment by name from Hopsworks Model Serving using the ms.get_deployment() method and then use the metadata object to perform the desired actions on it. Python # Get deployment by name deployment = ms . get_deployment ( \"awesome-deployment\" ) To learn what kind of operations can be done on a deployment, see the Deployment Guide .","title":"Model Serving"},{"location":"generated/model-serving/model_serving/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-serving/model_serving/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/model_serving/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/model_serving/#project_id","text":"Id of the project in which Model Serving is located. [source]","title":"project_id"},{"location":"generated/model-serving/model_serving/#project_name","text":"Name of the project in which Model Serving is located.","title":"project_name"},{"location":"generated/model-serving/model_serving/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/model_serving/#create_deployment","text":"ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source]","title":"create_deployment"},{"location":"generated/model-serving/model_serving/#create_predictor","text":"ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , model_server = None , serving_tool = None , script_file = None , resources = {}, inference_logger = {}, inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. model_server Optional[str] : Model server ran by the predictor. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object. [source]","title":"create_predictor"},{"location":"generated/model-serving/model_serving/#create_transformer","text":"ModelServing . create_transformer ( script_file = None , resources = {}) Create a Transformer metadata object. Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object. [source]","title":"create_transformer"},{"location":"generated/model-serving/model_serving/#get_deployment","text":"ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment"},{"location":"generated/model-serving/model_serving/#get_deployment_by_id","text":"ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment_by_id"},{"location":"generated/model-serving/model_serving/#get_deployments","text":"ModelServing . get_deployments () Get all deployments from model serving. Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving.","title":"get_deployments"},{"location":"generated/model-serving/model_serving_api/","text":"Model Serving # Retrieval # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on. Properties # [source] project_id # Id of the project in which Model Serving is located. [source] project_name # Name of the project in which Model Serving is located. Methods # [source] create_deployment # ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source] create_predictor # ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , model_server = None , serving_tool = None , script_file = None , resources = {}, inference_logger = {}, inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. model_server Optional[str] : Model server ran by the predictor. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object. [source] create_transformer # ModelServing . create_transformer ( script_file = None , resources = {}) Create a Transformer metadata object. Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object. [source] get_deployment # ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployment_by_id # ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source] get_deployments # ModelServing . get_deployments () Get all deployments from model serving. Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving.","title":"Model Serving"},{"location":"generated/model-serving/model_serving_api/#model-serving","text":"","title":"Model Serving"},{"location":"generated/model-serving/model_serving_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-serving/model_serving_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/model_serving_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/model_serving_api/#project_id","text":"Id of the project in which Model Serving is located. [source]","title":"project_id"},{"location":"generated/model-serving/model_serving_api/#project_name","text":"Name of the project in which Model Serving is located.","title":"project_name"},{"location":"generated/model-serving/model_serving_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/model_serving_api/#create_deployment","text":"ModelServing . create_deployment ( predictor , name = None ) Create a Deployment metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model. To create a deployment, call the save() method. Arguments predictor hsml.predictor.Predictor : predictor to be used in the deployment name Optional[str] : name of the deployment Returns Deployment . The model metadata object. [source]","title":"create_deployment"},{"location":"generated/model-serving/model_serving_api/#create_predictor","text":"ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , model_server = None , serving_tool = None , script_file = None , resources = {}, inference_logger = {}, inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. model_server Optional[str] : Model server ran by the predictor. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object. [source]","title":"create_predictor"},{"location":"generated/model-serving/model_serving_api/#create_transformer","text":"ModelServing . create_transformer ( script_file = None , resources = {}) Create a Transformer metadata object. Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object. [source]","title":"create_transformer"},{"location":"generated/model-serving/model_serving_api/#get_deployment","text":"ModelServing . get_deployment ( name ) Get a deployment by name from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments name str : Name of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment"},{"location":"generated/model-serving/model_serving_api/#get_deployment_by_id","text":"ModelServing . get_deployment_by_id ( id ) Get a deployment by id from Model Serving. Getting a deployment from Model Serving means getting its metadata handle so you can subsequently operate on it (e.g., start or stop). Arguments id int : Id of the deployment to get. Returns Deployment : The deployment metadata object. Raises RestAPIError : If unable to retrieve deployment from model serving. [source]","title":"get_deployment_by_id"},{"location":"generated/model-serving/model_serving_api/#get_deployments","text":"ModelServing . get_deployments () Get all deployments from model serving. Returns List[Deployment] : A list of deployments. Raises RestAPIError : If unable to retrieve deployments from model serving.","title":"get_deployments"},{"location":"generated/model-serving/predictor/","text":"Predictor # Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions. They can be configured to use different model servers, serving tools, log specific inference data or scale differently. To learn about all the options available see the Predictor Reference . See how to configure a predictor of a deployments in the Model Serving Quickstart . Model servers # Hopsworks Model Serving currently supports deploying models with a Flask server for python-based models or TensorFlow Serving for TensorFlow / Keras models. Support for TorchServe for running PyTorch models is coming soon. Today, you can deploy PyTorch models as python-based models. Show supported model servers Model Server Supported ML Frameworks Flask \u2705 python-based (scikit-learn, xgboost, pytorch...) TensorFlow Serving \u2705 keras, tensorflow TorchServe \u274c pytorch To learn how to specify the model server used in a deployment, see Model Server . Serving tools # In Hopsworks, model servers can be deployed in three different ways: directly on Docker, on Kubernetes deployments or using KServe inference services. Although the same models can be deployed in either of our two serving tools (Python or KServe), the use of KServe is highly recommended. The following is a comparitive table showing the features supported by each of them. Show serving tools comparison Feature / requirement Docker Kubernetes (enterprise) KServe (enterprise) Autoscaling (scale-out) \u274c \u2705 \u2705 Resource allocation \u2796 fixed \u2796 min. resources \u2705 min / max. resources Inference logging \u2796 simple \u2796 simple \u2705 fine-grained Inference batching \u2796 partially \u2796 partially \u2705 Scale-to-zero \u274c \u274c \u2705 after 30s of inactivity) Transformers \u274c \u274c \u2705 Low-latency predictions \u274c \u274c \u2705 Multiple models \u274c \u274c \u2796 (python-based) Custom predictor required (python-only) \u2705 \u2705 \u274c To learn how to specify which serving tool should be used for a deployment, see Serving Tool Reference . Custom predictor # Depending on the model server and serving tool used in the deployment, users can provide their own python script to load the model and make predictions. Show supported custom predictors Serving tool Model server Custom predictor script Docker Flask \u274c TensorFlow Serving \u2705 Kubernetes Flask \u274c TensorFlow Serving \u2705 KServe Flask \u2705 TensorFlow Serving \u2705 To configure a custom predictor, users must provide a python script implementing the following class. Python class Predict ( object ): def __init__ ( self ): \"\"\" Initialization code goes here: - Download the model artifact - Load the model \"\"\" pass def predict ( self , inputs ): \"\"\" Serve predictions using the trained model\"\"\" pass The predictor script should be available via a local file system path or a path on HopsFS. The path to this script then has to be provided when calling deploy() or create_predictor() methods. Find more details in the Predictor Reference . See examples of custom predictor scripts in the serving example notebooks . Configuration # Resource allocation # Depending on the combination of serving tool used to deploy the a model, resource allocation can be configured at different levels. While predictors on Docker containers only support a fixed number of resources (CPU and memory), using Kubernetes or KServe allows a better exploitation of the resources available in the platform, by enabling you to specify how many CPUs, GPUs, and memory are allocated to a deployment. Show supported resource allocation configuration Serving tool Component Resources Docker Predictor Fixed Transformer \u274c Kubernetes Predictor Minimum resources Transformer \u274c KServe Predictor Minimum / maximum resources Transformer Minimum / maximum resources To learn how to configure resource allocation for a deployment, see Resources Reference Inference logger # Once a model is deployed and starts making predictions as inference requests arrive, logging model inputs and predictions becomes essential to monitor the health of the model and take action if the model's performance degrades over time. Hopsworks supports logging both inference requests and predictions as events to a Kafka topic for analysis. To configure inference logging in a deployment, see Inference Logger Reference . The schema of Kafka events varies depending on the serving tool. Show kafka topic schemas KServe Docker / Kubernetes { \"fields\" : [ { \"name\" : \"servingId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceId\" , \"type\" : \"string\" }, { \"name\" : \"messageType\" , \"type\" : \"string\" }, { \"name\" : \"payload\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" } { \"fields\" : [ { \"name\" : \"modelId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceRequest\" , \"type\" : \"string\" }, { \"name\" : \"inferenceResponse\" , \"type\" : \"string\" }, { \"name\" : \"modelServer\" , \"type\" : \"string\" }, { \"name\" : \"servingTool\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" } Inference batcher # Depending on the serving tool and the model server, inference batching can be enabled to increase inference request throughput at the cost of higher latencies. To configure inference batching in a deployment, see Inference Batcher Reference . Show supported inference batcher configuration Serving tool Model server Inference batching Docker Flask \u274c TensorFlow Serving \u2705 Kubernetes Flask \u274c TensorFlow Serving \u2705 KServe Flask \u2705 TensorFlow Serving \u2705 Properties # [source] artifact_version # Artifact version deployed by the predictor. [source] created_at # Created at date of the predictor. [source] creator # Creator of the predictor. [source] id # Id of the predictor. [source] inference_batcher # Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source] inference_logger # Configuration of the inference logger attached to this predictor. [source] model_name # Name of the model deployed by the predictor. [source] model_path # Model path deployed by the predictor. [source] model_server # Model server used by the predictor. [source] model_version # Model version deployed by the predictor. [source] name # Name of the predictor. [source] requested_instances # Total number of requested instances in the predictor. [source] resources # Resource configuration for the deployment component (i.e., predictor or transformer). [source] script_file # Script file ran by the deployment component (i.e., predictor or transformer). [source] serving_tool # Serving tool used to run the model server. [source] transformer # Transformer configuration attached to the predictor. Methods # [source] deploy # Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Returns Deployment . The deployment metadata object. [source] describe # Predictor . describe () Print a description of the predictor [source] to_dict # Predictor . to_dict () To be implemented by the component type","title":"Predictor"},{"location":"generated/model-serving/predictor/#predictor","text":"Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions. They can be configured to use different model servers, serving tools, log specific inference data or scale differently. To learn about all the options available see the Predictor Reference . See how to configure a predictor of a deployments in the Model Serving Quickstart .","title":"Predictor"},{"location":"generated/model-serving/predictor/#model-servers","text":"Hopsworks Model Serving currently supports deploying models with a Flask server for python-based models or TensorFlow Serving for TensorFlow / Keras models. Support for TorchServe for running PyTorch models is coming soon. Today, you can deploy PyTorch models as python-based models. Show supported model servers Model Server Supported ML Frameworks Flask \u2705 python-based (scikit-learn, xgboost, pytorch...) TensorFlow Serving \u2705 keras, tensorflow TorchServe \u274c pytorch To learn how to specify the model server used in a deployment, see Model Server .","title":"Model servers"},{"location":"generated/model-serving/predictor/#serving-tools","text":"In Hopsworks, model servers can be deployed in three different ways: directly on Docker, on Kubernetes deployments or using KServe inference services. Although the same models can be deployed in either of our two serving tools (Python or KServe), the use of KServe is highly recommended. The following is a comparitive table showing the features supported by each of them. Show serving tools comparison Feature / requirement Docker Kubernetes (enterprise) KServe (enterprise) Autoscaling (scale-out) \u274c \u2705 \u2705 Resource allocation \u2796 fixed \u2796 min. resources \u2705 min / max. resources Inference logging \u2796 simple \u2796 simple \u2705 fine-grained Inference batching \u2796 partially \u2796 partially \u2705 Scale-to-zero \u274c \u274c \u2705 after 30s of inactivity) Transformers \u274c \u274c \u2705 Low-latency predictions \u274c \u274c \u2705 Multiple models \u274c \u274c \u2796 (python-based) Custom predictor required (python-only) \u2705 \u2705 \u274c To learn how to specify which serving tool should be used for a deployment, see Serving Tool Reference .","title":"Serving tools"},{"location":"generated/model-serving/predictor/#custom-predictor","text":"Depending on the model server and serving tool used in the deployment, users can provide their own python script to load the model and make predictions. Show supported custom predictors Serving tool Model server Custom predictor script Docker Flask \u274c TensorFlow Serving \u2705 Kubernetes Flask \u274c TensorFlow Serving \u2705 KServe Flask \u2705 TensorFlow Serving \u2705 To configure a custom predictor, users must provide a python script implementing the following class. Python class Predict ( object ): def __init__ ( self ): \"\"\" Initialization code goes here: - Download the model artifact - Load the model \"\"\" pass def predict ( self , inputs ): \"\"\" Serve predictions using the trained model\"\"\" pass The predictor script should be available via a local file system path or a path on HopsFS. The path to this script then has to be provided when calling deploy() or create_predictor() methods. Find more details in the Predictor Reference . See examples of custom predictor scripts in the serving example notebooks .","title":"Custom predictor"},{"location":"generated/model-serving/predictor/#configuration","text":"","title":"Configuration"},{"location":"generated/model-serving/predictor/#resource-allocation","text":"Depending on the combination of serving tool used to deploy the a model, resource allocation can be configured at different levels. While predictors on Docker containers only support a fixed number of resources (CPU and memory), using Kubernetes or KServe allows a better exploitation of the resources available in the platform, by enabling you to specify how many CPUs, GPUs, and memory are allocated to a deployment. Show supported resource allocation configuration Serving tool Component Resources Docker Predictor Fixed Transformer \u274c Kubernetes Predictor Minimum resources Transformer \u274c KServe Predictor Minimum / maximum resources Transformer Minimum / maximum resources To learn how to configure resource allocation for a deployment, see Resources Reference","title":"Resource allocation"},{"location":"generated/model-serving/predictor/#inference-logger","text":"Once a model is deployed and starts making predictions as inference requests arrive, logging model inputs and predictions becomes essential to monitor the health of the model and take action if the model's performance degrades over time. Hopsworks supports logging both inference requests and predictions as events to a Kafka topic for analysis. To configure inference logging in a deployment, see Inference Logger Reference . The schema of Kafka events varies depending on the serving tool. Show kafka topic schemas KServe Docker / Kubernetes { \"fields\" : [ { \"name\" : \"servingId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceId\" , \"type\" : \"string\" }, { \"name\" : \"messageType\" , \"type\" : \"string\" }, { \"name\" : \"payload\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" } { \"fields\" : [ { \"name\" : \"modelId\" , \"type\" : \"int\" }, { \"name\" : \"modelName\" , \"type\" : \"string\" }, { \"name\" : \"modelVersion\" , \"type\" : \"int\" }, { \"name\" : \"requestTimestamp\" , \"type\" : \"long\" }, { \"name\" : \"responseHttpCode\" , \"type\" : \"int\" }, { \"name\" : \"inferenceRequest\" , \"type\" : \"string\" }, { \"name\" : \"inferenceResponse\" , \"type\" : \"string\" }, { \"name\" : \"modelServer\" , \"type\" : \"string\" }, { \"name\" : \"servingTool\" , \"type\" : \"string\" } ], \"name\" : \"inferencelog\" , \"type\" : \"record\" }","title":"Inference logger"},{"location":"generated/model-serving/predictor/#inference-batcher","text":"Depending on the serving tool and the model server, inference batching can be enabled to increase inference request throughput at the cost of higher latencies. To configure inference batching in a deployment, see Inference Batcher Reference . Show supported inference batcher configuration Serving tool Model server Inference batching Docker Flask \u274c TensorFlow Serving \u2705 Kubernetes Flask \u274c TensorFlow Serving \u2705 KServe Flask \u2705 TensorFlow Serving \u2705","title":"Inference batcher"},{"location":"generated/model-serving/predictor/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/predictor/#artifact_version","text":"Artifact version deployed by the predictor. [source]","title":"artifact_version"},{"location":"generated/model-serving/predictor/#created_at","text":"Created at date of the predictor. [source]","title":"created_at"},{"location":"generated/model-serving/predictor/#creator","text":"Creator of the predictor. [source]","title":"creator"},{"location":"generated/model-serving/predictor/#id","text":"Id of the predictor. [source]","title":"id"},{"location":"generated/model-serving/predictor/#inference_batcher","text":"Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source]","title":"inference_batcher"},{"location":"generated/model-serving/predictor/#inference_logger","text":"Configuration of the inference logger attached to this predictor. [source]","title":"inference_logger"},{"location":"generated/model-serving/predictor/#model_name","text":"Name of the model deployed by the predictor. [source]","title":"model_name"},{"location":"generated/model-serving/predictor/#model_path","text":"Model path deployed by the predictor. [source]","title":"model_path"},{"location":"generated/model-serving/predictor/#model_server","text":"Model server used by the predictor. [source]","title":"model_server"},{"location":"generated/model-serving/predictor/#model_version","text":"Model version deployed by the predictor. [source]","title":"model_version"},{"location":"generated/model-serving/predictor/#name","text":"Name of the predictor. [source]","title":"name"},{"location":"generated/model-serving/predictor/#requested_instances","text":"Total number of requested instances in the predictor. [source]","title":"requested_instances"},{"location":"generated/model-serving/predictor/#resources","text":"Resource configuration for the deployment component (i.e., predictor or transformer). [source]","title":"resources"},{"location":"generated/model-serving/predictor/#script_file","text":"Script file ran by the deployment component (i.e., predictor or transformer). [source]","title":"script_file"},{"location":"generated/model-serving/predictor/#serving_tool","text":"Serving tool used to run the model server. [source]","title":"serving_tool"},{"location":"generated/model-serving/predictor/#transformer","text":"Transformer configuration attached to the predictor.","title":"transformer"},{"location":"generated/model-serving/predictor/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/predictor/#deploy","text":"Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Returns Deployment . The deployment metadata object. [source]","title":"deploy"},{"location":"generated/model-serving/predictor/#describe","text":"Predictor . describe () Print a description of the predictor [source]","title":"describe"},{"location":"generated/model-serving/predictor/#to_dict","text":"Predictor . to_dict () To be implemented by the component type","title":"to_dict"},{"location":"generated/model-serving/predictor_api/","text":"Predictor # Handle # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on. Creation # [source] create_predictor # ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , model_server = None , serving_tool = None , script_file = None , resources = {}, inference_logger = {}, inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. model_server Optional[str] : Model server ran by the predictor. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object. Retrieval # deployment.predictor # Predictors can be accessed from the deployment metadata objects. deployment . predictor To retrieve a deployment, see the Deployment Reference . Properties # [source] artifact_version # Artifact version deployed by the predictor. [source] created_at # Created at date of the predictor. [source] creator # Creator of the predictor. [source] id # Id of the predictor. [source] inference_batcher # Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source] inference_logger # Configuration of the inference logger attached to this predictor. [source] model_name # Name of the model deployed by the predictor. [source] model_path # Model path deployed by the predictor. [source] model_server # Model server used by the predictor. [source] model_version # Model version deployed by the predictor. [source] name # Name of the predictor. [source] requested_instances # Total number of requested instances in the predictor. [source] resources # Resource configuration for the deployment component (i.e., predictor or transformer). [source] script_file # Script file ran by the deployment component (i.e., predictor or transformer). [source] serving_tool # Serving tool used to run the model server. [source] transformer # Transformer configuration attached to the predictor. Methods # [source] deploy # Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Returns Deployment . The deployment metadata object. [source] describe # Predictor . describe () Print a description of the predictor [source] to_dict # Predictor . to_dict () To be implemented by the component type","title":"Predictor"},{"location":"generated/model-serving/predictor_api/#predictor","text":"","title":"Predictor"},{"location":"generated/model-serving/predictor_api/#handle","text":"[source]","title":"Handle"},{"location":"generated/model-serving/predictor_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/predictor_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/predictor_api/#create_predictor","text":"ModelServing . create_predictor ( model , name = None , artifact_version = \"CREATE\" , model_server = None , serving_tool = None , script_file = None , resources = {}, inference_logger = {}, inference_batcher = None , transformer = None , ) Create a Predictor metadata object. Lazy This method is lazy and does not persist any metadata or deploy any model on its own. To create a deployment using this predictor, call the deploy() method. Arguments model hsml.model.Model : Model to be deployed. name Optional[str] : Name of the predictor. artifact_version Optional[str] : Version number of the model artifact to deploy, CREATE to create a new model artifact or MODEL-ONLY to reuse the shared artifact containing only the model files. model_server Optional[str] : Model server ran by the predictor. serving_tool Optional[str] : Serving tool used to deploy the model server. script_file Optional[str] : Path to a custom predictor script implementing the Predict class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the predictor. inference_logger Optional[Union[hsml.inference_logger.InferenceLogger, dict, str]] : Inference logger configuration. inference_batcher Optional[Union[hsml.inference_batcher.InferenceBatcher, dict]] : Inference batcher configuration. transformer Optional[Union[hsml.transformer.Transformer, dict]] : Transformer to be deployed together with the predictor. Returns Predictor . The predictor metadata object.","title":"create_predictor"},{"location":"generated/model-serving/predictor_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/predictor_api/#deploymentpredictor","text":"Predictors can be accessed from the deployment metadata objects. deployment . predictor To retrieve a deployment, see the Deployment Reference .","title":"deployment.predictor"},{"location":"generated/model-serving/predictor_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/predictor_api/#artifact_version","text":"Artifact version deployed by the predictor. [source]","title":"artifact_version"},{"location":"generated/model-serving/predictor_api/#created_at","text":"Created at date of the predictor. [source]","title":"created_at"},{"location":"generated/model-serving/predictor_api/#creator","text":"Creator of the predictor. [source]","title":"creator"},{"location":"generated/model-serving/predictor_api/#id","text":"Id of the predictor. [source]","title":"id"},{"location":"generated/model-serving/predictor_api/#inference_batcher","text":"Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source]","title":"inference_batcher"},{"location":"generated/model-serving/predictor_api/#inference_logger","text":"Configuration of the inference logger attached to this predictor. [source]","title":"inference_logger"},{"location":"generated/model-serving/predictor_api/#model_name","text":"Name of the model deployed by the predictor. [source]","title":"model_name"},{"location":"generated/model-serving/predictor_api/#model_path","text":"Model path deployed by the predictor. [source]","title":"model_path"},{"location":"generated/model-serving/predictor_api/#model_server","text":"Model server used by the predictor. [source]","title":"model_server"},{"location":"generated/model-serving/predictor_api/#model_version","text":"Model version deployed by the predictor. [source]","title":"model_version"},{"location":"generated/model-serving/predictor_api/#name","text":"Name of the predictor. [source]","title":"name"},{"location":"generated/model-serving/predictor_api/#requested_instances","text":"Total number of requested instances in the predictor. [source]","title":"requested_instances"},{"location":"generated/model-serving/predictor_api/#resources","text":"Resource configuration for the deployment component (i.e., predictor or transformer). [source]","title":"resources"},{"location":"generated/model-serving/predictor_api/#script_file","text":"Script file ran by the deployment component (i.e., predictor or transformer). [source]","title":"script_file"},{"location":"generated/model-serving/predictor_api/#serving_tool","text":"Serving tool used to run the model server. [source]","title":"serving_tool"},{"location":"generated/model-serving/predictor_api/#transformer","text":"Transformer configuration attached to the predictor.","title":"transformer"},{"location":"generated/model-serving/predictor_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/predictor_api/#deploy","text":"Predictor . deploy () Create a deployment for this predictor and persists it in the Model Serving. Returns Deployment . The deployment metadata object. [source]","title":"deploy"},{"location":"generated/model-serving/predictor_api/#describe","text":"Predictor . describe () Print a description of the predictor [source]","title":"describe"},{"location":"generated/model-serving/predictor_api/#to_dict","text":"Predictor . to_dict () To be implemented by the component type","title":"to_dict"},{"location":"generated/model-serving/predictor_state_api/","text":"Deployment state # The status of a deployment corresponds to the status of the predictor configured within it. Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon. Retrieval # [source] get_state # Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment. Properties # [source] available_predictor_instances # Available predicotr instances. [source] available_transformer_instances # Available transformer instances. [source] conditions # Conditions of the current state of predictor. [source] deployed # Whether the predictor is deployed or not. [source] external_ip # External IP for the predictor. [source] external_port # External port for the predictor. [source] internal_ips # Internal IPs for the predictor. [source] internal_path # Internal path for the predictor. [source] internal_port # Internal port for the predictor. [source] revision # Last revision of the predictor. [source] status # Status of the predictor. Methods # [source] describe # PredictorState . describe () Print a description of the deployment state [source] to_dict # PredictorState . to_dict ()","title":"Deployment state"},{"location":"generated/model-serving/predictor_state_api/#deployment-state","text":"The status of a deployment corresponds to the status of the predictor configured within it. Note Currently, only one predictor is supported in a deployment. Support for multiple predictors (the inference graphs) is coming soon.","title":"Deployment state"},{"location":"generated/model-serving/predictor_state_api/#retrieval","text":"[source]","title":"Retrieval"},{"location":"generated/model-serving/predictor_state_api/#get_state","text":"Deployment . get_state () Get the current state of the deployment Returns PredictorState . The state of the deployment.","title":"get_state"},{"location":"generated/model-serving/predictor_state_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/predictor_state_api/#available_predictor_instances","text":"Available predicotr instances. [source]","title":"available_predictor_instances"},{"location":"generated/model-serving/predictor_state_api/#available_transformer_instances","text":"Available transformer instances. [source]","title":"available_transformer_instances"},{"location":"generated/model-serving/predictor_state_api/#conditions","text":"Conditions of the current state of predictor. [source]","title":"conditions"},{"location":"generated/model-serving/predictor_state_api/#deployed","text":"Whether the predictor is deployed or not. [source]","title":"deployed"},{"location":"generated/model-serving/predictor_state_api/#external_ip","text":"External IP for the predictor. [source]","title":"external_ip"},{"location":"generated/model-serving/predictor_state_api/#external_port","text":"External port for the predictor. [source]","title":"external_port"},{"location":"generated/model-serving/predictor_state_api/#internal_ips","text":"Internal IPs for the predictor. [source]","title":"internal_ips"},{"location":"generated/model-serving/predictor_state_api/#internal_path","text":"Internal path for the predictor. [source]","title":"internal_path"},{"location":"generated/model-serving/predictor_state_api/#internal_port","text":"Internal port for the predictor. [source]","title":"internal_port"},{"location":"generated/model-serving/predictor_state_api/#revision","text":"Last revision of the predictor. [source]","title":"revision"},{"location":"generated/model-serving/predictor_state_api/#status","text":"Status of the predictor.","title":"status"},{"location":"generated/model-serving/predictor_state_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/predictor_state_api/#describe","text":"PredictorState . describe () Print a description of the deployment state [source]","title":"describe"},{"location":"generated/model-serving/predictor_state_api/#to_dict","text":"PredictorState . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/project/","text":"Project/Connection # In Hopsworks a Project is a sandboxed collection of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Model Management (Registry and Serving). However, it is possible to share a Model Registry and a Model Serving among projects. When working with the Model Management (Registry and Serving) from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Model Registries and Model Servings simultaneously. The connection object to a Hopsworks instance is represented by a Connection object . The handle can then be used to retrieve a reference to the Model Management ( Registry and Serving ) you want to operate on. Examples # Python Connecting from Hopsworks import hsml conn = hsml . connection () Connecting from Python environment To connect from an external Python environment you have to provide the api key, project name, and hostname for your Hopsworks cluster. Here, we pass the api key with the insecure api_key_value parameter: import hsml conn = hsml . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_ml_admin000\" , hostname_verification = False , api_key_value = \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" ) A more secure approach for passing the API key is as a file: import hsml conn = hsml . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_ml_admin000\" , hostname_verification = False , api_key_file = \"modelregistry.key\" ) Getting the Model Management (Registry and Serving) handle mr = conn . get_model_registry () ms = conn . get_model_serving () Sharing a Model Registry # Connections are on a project-level, however, it is possible to share model registries among projects, so even if you have a connection to one project, you can retrieve a handle to any model registry shared with that project. To share a model registry, you should follow these steps: Sharing a Model Registry Open the project of the model registry that you would like to share on Hopsworks. Go to the Data Set browser and right click the Models entry. Click Share with , then select Project and choose the project you wish to share the model registry with. Select the permissions level that the project user members should have on the model registry and click Share . Open the project you just shared the model registry with. Go to the Data Sets browser and there you should see the shared model registry as [project_name_of_shared_model_registry]::Models . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this model registry from the other project. Sharing a model registry between projects Accepting a shared model registry from a project Connection Handle # [source] Connection # hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project. Methods # [source] close # Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source] connect # Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source] get_model_registry # Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"Project/Connection"},{"location":"generated/model-serving/project/#projectconnection","text":"In Hopsworks a Project is a sandboxed collection of users, data, and programs (where data can be shared in a controlled manner between projects). Each Project can have its own Model Management (Registry and Serving). However, it is possible to share a Model Registry and a Model Serving among projects. When working with the Model Management (Registry and Serving) from a programming environment you can connect to a single Hopsworks instance at a time, but it is possible to access multiple Model Registries and Model Servings simultaneously. The connection object to a Hopsworks instance is represented by a Connection object . The handle can then be used to retrieve a reference to the Model Management ( Registry and Serving ) you want to operate on.","title":"Project/Connection"},{"location":"generated/model-serving/project/#examples","text":"Python Connecting from Hopsworks import hsml conn = hsml . connection () Connecting from Python environment To connect from an external Python environment you have to provide the api key, project name, and hostname for your Hopsworks cluster. Here, we pass the api key with the insecure api_key_value parameter: import hsml conn = hsml . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_ml_admin000\" , hostname_verification = False , api_key_value = \"PFcy3dZ6wLXYglRd.ydcdq5jH878IdG7xlL9lHVqrS8v3sBUqQgyR4xbpUgDnB5ZpYro6O\" ) A more secure approach for passing the API key is as a file: import hsml conn = hsml . connection ( host = \"ec2-13-53-124-128.eu-north-1.compute.amazonaws.com\" , project = \"demo_ml_admin000\" , hostname_verification = False , api_key_file = \"modelregistry.key\" ) Getting the Model Management (Registry and Serving) handle mr = conn . get_model_registry () ms = conn . get_model_serving ()","title":"Examples"},{"location":"generated/model-serving/project/#sharing-a-model-registry","text":"Connections are on a project-level, however, it is possible to share model registries among projects, so even if you have a connection to one project, you can retrieve a handle to any model registry shared with that project. To share a model registry, you should follow these steps: Sharing a Model Registry Open the project of the model registry that you would like to share on Hopsworks. Go to the Data Set browser and right click the Models entry. Click Share with , then select Project and choose the project you wish to share the model registry with. Select the permissions level that the project user members should have on the model registry and click Share . Open the project you just shared the model registry with. Go to the Data Sets browser and there you should see the shared model registry as [project_name_of_shared_model_registry]::Models . Click this entry, you will be asked to accept this shared Dataset, click Accept . You should now have access to this model registry from the other project. Sharing a model registry between projects Accepting a shared model registry from a project","title":"Sharing a Model Registry"},{"location":"generated/model-serving/project/#connection-handle","text":"[source]","title":"Connection Handle"},{"location":"generated/model-serving/project/#connection","text":"hsml . connection . Connection ( host = None , port = 443 , project = None , hostname_verification = True , trust_store_path = None , api_key_file = None , api_key_value = None , ) A Hopsworks Model Management connection object. The connection is project specific, so you can access the project's own Model Registry and Model Serving. This class provides convenience classmethods accessible from the hsml -module: Connection factory For convenience, hsml provides a factory method, accessible from the top level module, so you don't have to import the Connection class manually: import hsml conn = hsml . connection () Save API Key as File To get started quickly, you can simply create a file with the previously created Hopsworks API Key and place it on the environment from which you wish to connect to Hopsworks. You can then connect by simply passing the path to the key file when instantiating a connection: import hsml conn = hsml . connection ( 'my_instance' , # DNS of your Hopsworks instance 443 , # Port to reach your Hopsworks instance, defaults to 443 'my_project' , # Name of your Hopsworks project api_key_file = 'modelregistry.key' , # The file containing the API key generated above hostname_verification = True ) # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Clients in external clusters need to connect to the Hopsworks Model Registry and Model Serving using an API key. The API key is generated inside the Hopsworks platform, and requires at least the \"project\", \"modelregistry\", \"dataset.create\", \"dataset.view\", \"dataset.delete\", \"serving\" and \"kafka\" scopes to be able to access a model registry and its model serving. For more information, see the integration guides . Arguments host Optional[str] : The hostname of the Hopsworks instance, defaults to None . port int : The port on which the Hopsworks instance can be reached, defaults to 443 . project Optional[str] : The name of the project to connect to. When running on Hopsworks, this defaults to the project from where the client is run from. Defaults to None . hostname_verification bool : Whether or not to verify Hopsworks certificate, defaults to True . trust_store_path Optional[str] : Path on the file system containing the Hopsworks certificates, defaults to None . api_key_file Optional[str] : Path to a file containing the API Key. api_key_value Optional[str] : API Key as string, if provided, however, this should be used with care, especially if the used notebook or job script is accessible by multiple parties. Defaults to None . Returns Connection . Connection handle to perform operations on a Hopsworks project.","title":"Connection"},{"location":"generated/model-serving/project/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/project/#close","text":"Connection . close () Close a connection gracefully. This will clean up any materialized certificates on the local file system of external environments. Usage is recommended but optional. [source]","title":"close"},{"location":"generated/model-serving/project/#connect","text":"Connection . connect () Instantiate the connection. Creating a Connection object implicitly calls this method for you to instantiate the connection. However, it is possible to close the connection gracefully with the close() method, in order to clean up materialized certificates. This might be desired when working on external environments. Subsequently you can call connect() again to reopen the connection. Example import hsml conn = hsml . connection () conn . close () conn . connect () [source]","title":"connect"},{"location":"generated/model-serving/project/#get_model_registry","text":"Connection . get_model_registry ( project = None ) Get a reference to a model registry to perform operations on, defaulting to the project's default model registry. Shared model registries can be retrieved by passing the project argument. Arguments project str : The name of the project that owns the shared model registry, the model registry must be shared with the project the connection was established for, defaults to None . Returns ModelRegistry . A model registry handle object to perform operations on. [source]","title":"get_model_registry"},{"location":"generated/model-serving/project/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/resources_api/","text":"Resources # Creation # [source] Resources # hsml . resources . Resources ( num_instances = None , cores = None , memory = None , gpus = None ) Resource configuration for a predictor or transformer. Arguments num_instances Optional[int] : Number of instances. The default value is 1 instance. cores Optional[int] : Number of CPUs. The default value is 1 CPUs. memory Optional[int] : Memory resources. The default value is 1024Mb . gpus Optional[int] : Number of GPUs. The default value is 0 GPUs. Returns Resources . Resource configuration for a predictor or transformer. Retrieval # predictor.resources # Resources allocated for a preditor can be accessed from the predictor metadata object. predictor . resources Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference . transformer.resources # Resources allocated for a transformer can be accessed from the transformer metadata object. transformer . resources Transformer can be found in the predictor metadata objects (see Predictor Reference ). Properties # [source] cores # Number of CPUs to be allocated per instance [source] gpus # Number of GPUs to be allocated per instance [source] memory # Memory resources to be allocated per instance [source] num_instances # Number of instances Methods # [source] describe # Resources . describe () Print a description of the resource configuration [source] to_dict # Resources . to_dict ()","title":"Resources"},{"location":"generated/model-serving/resources_api/#resources","text":"","title":"Resources"},{"location":"generated/model-serving/resources_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/resources_api/#resources_1","text":"hsml . resources . Resources ( num_instances = None , cores = None , memory = None , gpus = None ) Resource configuration for a predictor or transformer. Arguments num_instances Optional[int] : Number of instances. The default value is 1 instance. cores Optional[int] : Number of CPUs. The default value is 1 CPUs. memory Optional[int] : Memory resources. The default value is 1024Mb . gpus Optional[int] : Number of GPUs. The default value is 0 GPUs. Returns Resources . Resource configuration for a predictor or transformer.","title":"Resources"},{"location":"generated/model-serving/resources_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/resources_api/#predictorresources","text":"Resources allocated for a preditor can be accessed from the predictor metadata object. predictor . resources Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference .","title":"predictor.resources"},{"location":"generated/model-serving/resources_api/#transformerresources","text":"Resources allocated for a transformer can be accessed from the transformer metadata object. transformer . resources Transformer can be found in the predictor metadata objects (see Predictor Reference ).","title":"transformer.resources"},{"location":"generated/model-serving/resources_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/resources_api/#cores","text":"Number of CPUs to be allocated per instance [source]","title":"cores"},{"location":"generated/model-serving/resources_api/#gpus","text":"Number of GPUs to be allocated per instance [source]","title":"gpus"},{"location":"generated/model-serving/resources_api/#memory","text":"Memory resources to be allocated per instance [source]","title":"memory"},{"location":"generated/model-serving/resources_api/#num_instances","text":"Number of instances","title":"num_instances"},{"location":"generated/model-serving/resources_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/resources_api/#describe","text":"Resources . describe () Print a description of the resource configuration [source]","title":"describe"},{"location":"generated/model-serving/resources_api/#to_dict","text":"Resources . to_dict ()","title":"to_dict"},{"location":"generated/model-serving/transformer/","text":"Transformer # Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. They run on a built-in Flask server provided by Hopsworks and require a custom python script implementing the following class. Info Transformers are only supported in deployments using KServe as serving tool. Python class Transformer ( object ): def __init__ ( self ): \"\"\" Initialization code goes here \"\"\" pass def preprocess ( self , inputs ): \"\"\" Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. \"\"\" return inputs def postprocess ( self , outputs ): \"\"\" Transform the predictions computed by the model before returning a response \"\"\" return outputs The path to this script (local or HopsFS) has to be provided when calling the create_transformer() . Find more details in the Transformer Reference . See examples of transformer scripts in the serving example notebooks . Properties # [source] resources # Resource configuration for the deployment component (i.e., predictor or transformer). [source] script_file # Script file ran by the deployment component (i.e., predictor or transformer). Methods # [source] describe # Transformer . describe () Print a description of the transformer [source] to_dict # Transformer . to_dict () To be implemented by the component type","title":"Transformer"},{"location":"generated/model-serving/transformer/#transformer","text":"Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model. They run on a built-in Flask server provided by Hopsworks and require a custom python script implementing the following class. Info Transformers are only supported in deployments using KServe as serving tool. Python class Transformer ( object ): def __init__ ( self ): \"\"\" Initialization code goes here \"\"\" pass def preprocess ( self , inputs ): \"\"\" Transform the requests inputs here. The object returned by this method will be used as model input to make predictions. \"\"\" return inputs def postprocess ( self , outputs ): \"\"\" Transform the predictions computed by the model before returning a response \"\"\" return outputs The path to this script (local or HopsFS) has to be provided when calling the create_transformer() . Find more details in the Transformer Reference . See examples of transformer scripts in the serving example notebooks .","title":"Transformer"},{"location":"generated/model-serving/transformer/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/transformer/#resources","text":"Resource configuration for the deployment component (i.e., predictor or transformer). [source]","title":"resources"},{"location":"generated/model-serving/transformer/#script_file","text":"Script file ran by the deployment component (i.e., predictor or transformer).","title":"script_file"},{"location":"generated/model-serving/transformer/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/transformer/#describe","text":"Transformer . describe () Print a description of the transformer [source]","title":"describe"},{"location":"generated/model-serving/transformer/#to_dict","text":"Transformer . to_dict () To be implemented by the component type","title":"to_dict"},{"location":"generated/model-serving/transformer_api/","text":"Transformer # Handle # [source] get_model_serving # Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on. Creation # [source] create_transformer # ModelServing . create_transformer ( script_file = None , resources = {}) Create a Transformer metadata object. Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object. Retrieval # predictor.transformer # Transformers can be accessed from the predictor metadata objects. predictor . transformer Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference . Properties # [source] inference_batcher # Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source] resources # Resource configuration for the deployment component (i.e., predictor or transformer). [source] script_file # Script file ran by the deployment component (i.e., predictor or transformer). Methods # [source] describe # Transformer . describe () Print a description of the transformer [source] to_dict # Transformer . to_dict () To be implemented by the component type","title":"Transformer"},{"location":"generated/model-serving/transformer_api/#transformer","text":"","title":"Transformer"},{"location":"generated/model-serving/transformer_api/#handle","text":"[source]","title":"Handle"},{"location":"generated/model-serving/transformer_api/#get_model_serving","text":"Connection . get_model_serving () Get a reference to model serving to perform operations on. Model serving operates on top of a model registry, defaulting to the project's default model registry. Returns ModelServing . A model serving handle object to perform operations on.","title":"get_model_serving"},{"location":"generated/model-serving/transformer_api/#creation","text":"[source]","title":"Creation"},{"location":"generated/model-serving/transformer_api/#create_transformer","text":"ModelServing . create_transformer ( script_file = None , resources = {}) Create a Transformer metadata object. Lazy This method is lazy and does not persist any metadata or deploy any transformer. To create a deployment using this transformer, set it in the predictor.transformer property. Arguments script_file Optional[str] : Path to a custom predictor script implementing the Transformer class. resources Optional[Union[hsml.resources.PredictorResources, dict]] : Resources to be allocated for the transformer. Returns Transformer . The model metadata object.","title":"create_transformer"},{"location":"generated/model-serving/transformer_api/#retrieval","text":"","title":"Retrieval"},{"location":"generated/model-serving/transformer_api/#predictortransformer","text":"Transformers can be accessed from the predictor metadata objects. predictor . transformer Predictors can be found in the deployment metadata objects (see Predictor Reference ). To retrieve a deployment, see the Deployment Reference .","title":"predictor.transformer"},{"location":"generated/model-serving/transformer_api/#properties","text":"[source]","title":"Properties"},{"location":"generated/model-serving/transformer_api/#inference_batcher","text":"Configuration of the inference batcher attached to the deployment component (i.e., predictor or transformer). [source]","title":"inference_batcher"},{"location":"generated/model-serving/transformer_api/#resources","text":"Resource configuration for the deployment component (i.e., predictor or transformer). [source]","title":"resources"},{"location":"generated/model-serving/transformer_api/#script_file","text":"Script file ran by the deployment component (i.e., predictor or transformer).","title":"script_file"},{"location":"generated/model-serving/transformer_api/#methods","text":"[source]","title":"Methods"},{"location":"generated/model-serving/transformer_api/#describe","text":"Transformer . describe () Print a description of the transformer [source]","title":"describe"},{"location":"generated/model-serving/transformer_api/#to_dict","text":"Transformer . to_dict () To be implemented by the component type","title":"to_dict"},{"location":"integrations/overview/","text":"Hopsworks # If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started guides for Model Registry and Model Serving . External Python environment (Local) # Connecting to the Model Management (Registry and Serving) from any Python environment, such as your local environment, requires setting up a Model Management API Key and installing the HSML Python client library. The Python integration guide explains step by step how to connect to the Model Management from any Python environment.","title":"Overview"},{"location":"integrations/overview/#hopsworks","text":"If you are using Spark or Python within Hopsworks, there is no further configuration required. Head over to the Getting Started guides for Model Registry and Model Serving .","title":"Hopsworks"},{"location":"integrations/overview/#external-python-environment-local","text":"Connecting to the Model Management (Registry and Serving) from any Python environment, such as your local environment, requires setting up a Model Management API Key and installing the HSML Python client library. The Python integration guide explains step by step how to connect to the Model Management from any Python environment.","title":"External Python environment (Local)"},{"location":"integrations/python/","text":"Python Environments # Connecting to the Model Management (Registry and Serving) from any Python environment requires setting up a Model Management API key and installing the library. This guide explains step by step how to connect to the Model Management from any Python environment such as the one on your laptop. Generate an API key # In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project , modelregistry , dataset.create , dataset.view , dataset.delete , serving , kafka scopes before creating the key. Copy the key into your clipboard. More documentation can be found here . Create a file called modelmanagement.key in your designated Python environment and save the API key from your clipboard in the file. Scopes The API key should contain at least the following scopes: project modelregistry dataset.create dataset.view dataset.delete serving kafka API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one. Install HSML # To be able to access the Hopsworks Model Management (Registry and Serving), the HSML Python library needs to be installed in the environment from which you want to connect to the Model Management. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . Matching Hopsworks version The major and minor version of HSML needs to match the major and minor version of Hopsworks . For example for a Hopsworks cluster running with version 2.5.0, the following installation command will install the latest release available for HSML. pip install hsml==2.5.* You find the Hopsworks version inside any of your Project's settings tab on Hopsworks Connect to the Model Registry and Model Serving # You are now ready to connect to the Hopsworks Model Registry and Model Serving from your Python environment: import hsml conn = hsml . connection ( host = 'my_instance' , # DNS of your Model Registry instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Model Registry project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Ports If you are unable to connect, please ensure that your Model Management can receive incoming traffic from your Python environment on port 443. Next Steps # For more information about how to connect, see the Connection guide.","title":"Python"},{"location":"integrations/python/#python-environments","text":"Connecting to the Model Management (Registry and Serving) from any Python environment requires setting up a Model Management API key and installing the library. This guide explains step by step how to connect to the Model Management from any Python environment such as the one on your laptop.","title":"Python Environments"},{"location":"integrations/python/#generate-an-api-key","text":"In Hopsworks, click on your username in the top-right corner and select Settings to open the user settings. Select API keys . Give the key a name and select the project , modelregistry , dataset.create , dataset.view , dataset.delete , serving , kafka scopes before creating the key. Copy the key into your clipboard. More documentation can be found here . Create a file called modelmanagement.key in your designated Python environment and save the API key from your clipboard in the file. Scopes The API key should contain at least the following scopes: project modelregistry dataset.create dataset.view dataset.delete serving kafka API keys can be created in the User Settings on Hopsworks Info You are only able to retrieve the API key once. If you did not manage to copy it to your clipboard, delete it and create a new one.","title":"Generate an API key"},{"location":"integrations/python/#install-hsml","text":"To be able to access the Hopsworks Model Management (Registry and Serving), the HSML Python library needs to be installed in the environment from which you want to connect to the Model Management. You can install the library through pip. We recommend using a Python environment manager such as virtualenv or conda . Matching Hopsworks version The major and minor version of HSML needs to match the major and minor version of Hopsworks . For example for a Hopsworks cluster running with version 2.5.0, the following installation command will install the latest release available for HSML. pip install hsml==2.5.* You find the Hopsworks version inside any of your Project's settings tab on Hopsworks","title":"Install HSML"},{"location":"integrations/python/#connect-to-the-model-registry-and-model-serving","text":"You are now ready to connect to the Hopsworks Model Registry and Model Serving from your Python environment: import hsml conn = hsml . connection ( host = 'my_instance' , # DNS of your Model Registry instance port = 443 , # Port to reach your Hopsworks instance, defaults to 443 project = 'my_project' , # Name of your Hopsworks Model Registry project api_key_value = 'apikey' , # The API key to authenticate with Hopsworks hostname_verification = True # Disable for self-signed certificates ) mr = conn . get_model_registry () # Get the project's default model registry ms = conn . get_model_serving () # Uses the previous model registry Ports If you are unable to connect, please ensure that your Model Management can receive incoming traffic from your Python environment on port 443.","title":"Connect to the Model Registry and Model Serving"},{"location":"integrations/python/#next-steps","text":"For more information about how to connect, see the Connection guide.","title":"Next Steps"},{"location":"model-registry/quickstart/","text":"Quickstart Guide # Hopsworks Model Registry is a centralized repository, within an organization, to manage machine learning models. A model is the product of training a machine learning algorithm with training data. It could be an image classifier used to detect objects in an image, such as for example detecting cancer in an MRI scan. In this Quickstart Guide we are going to focus on how data scientists can create models and publish them to the Model Registry to make them available to make them available for download by batch applications, model serving infrastructure, and other use cases. HSML library # The Hopsworks Model Registry library is part of hsml ( H opswork s M achine L earning). The library is Apache V2 licensed and available here . It currently comes with a Python SDK. If you want to connect to the Model Registry from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within the Model Registry. You can modify metadata by changing it in the metadata-objects and subsequently persisting it to the Model Registry. In fact, the Model Registry itself is also represented by an object. Furthermore, these objects have methods to save model artifacts along with the entities in the Model Registry. Guide Notebooks # This guide is based on a series of notebooks , which are available in the Deep Learning Demo Tour Project on Hopsworks. Connection, Project and Model Registry # The first step is to establish a connection with your Hopsworks instance and retrieve the object that represents the Model Registry you'll be working with. By default connection.get_model_registry() returns the Model Registry of the project you are working with. However, it accepts also a project name as parameter to select a different Model Registry. Python import hsml # Create a connection connection = hsml . connection () # Get the model registry handle for the project's model registry mr = connection . get_model_registry () Models # Assuming you have done some model training, and exported a model to a directory on a local file path, the model artifacts and additional metadata can now be saved to the Model Registry. See the example notebooks . Creation # Create a model named mnist . As you can see, you have the possibility to set parameters for the Model, such as the version number, or metrics which is set to attach model training metrics on the model. The Model Guide guides through the full configuration of Models. Python mnist_model_meta = mr . tensorflow . create_model ( name = \"mnist\" , version = 1 , metrics = { \"accuracy\" : 0.94 }, description = \"mnist model description\" ) Up to this point we have just created the metadata object representing the model. However, we haven't saved the model in the model registry yet. To do so, we can call the method save on the metadata object created in the cell above. The save method takes a single parameter which is the path to either the model file or the directory on the local filesystem containing multiple model files. Python mnist_model_meta . save ( \"/tmp/model_directory\" ) # or /tmp/model_file Retrieval # If there were models previously created in your Model Registry, or you want to pick up where you left off before, you can retrieve and read models in a similar fashion as creating them: Using the Model Registry object, you can retrieve handles to the entities, such as models, in the Model Registry. By default, this will return the first version of an entity, if you want a more recent version, you need to specify the version. Python mnist_model_meta = mr . get_model ( 'mnist' , version = 1 ) # Download the model model_download_path = mnist_model_meta . download () # Load the model tf . saved_model . load ( model_download_path ) To seamlessly combine HSML with model serving components, the library makes it simple to query for the best performing model. In this instance, we get the best model version by querying for the model version with the highest accuracy metric attached. Python mnist_model_meta = mr . get_best_model ( 'mnist' , 'accuracy' , 'max' )","title":"Quickstart"},{"location":"model-registry/quickstart/#quickstart-guide","text":"Hopsworks Model Registry is a centralized repository, within an organization, to manage machine learning models. A model is the product of training a machine learning algorithm with training data. It could be an image classifier used to detect objects in an image, such as for example detecting cancer in an MRI scan. In this Quickstart Guide we are going to focus on how data scientists can create models and publish them to the Model Registry to make them available to make them available for download by batch applications, model serving infrastructure, and other use cases.","title":"Quickstart Guide"},{"location":"model-registry/quickstart/#hsml-library","text":"The Hopsworks Model Registry library is part of hsml ( H opswork s M achine L earning). The library is Apache V2 licensed and available here . It currently comes with a Python SDK. If you want to connect to the Model Registry from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within the Model Registry. You can modify metadata by changing it in the metadata-objects and subsequently persisting it to the Model Registry. In fact, the Model Registry itself is also represented by an object. Furthermore, these objects have methods to save model artifacts along with the entities in the Model Registry.","title":"HSML library"},{"location":"model-registry/quickstart/#guide-notebooks","text":"This guide is based on a series of notebooks , which are available in the Deep Learning Demo Tour Project on Hopsworks.","title":"Guide Notebooks"},{"location":"model-registry/quickstart/#connection-project-and-model-registry","text":"The first step is to establish a connection with your Hopsworks instance and retrieve the object that represents the Model Registry you'll be working with. By default connection.get_model_registry() returns the Model Registry of the project you are working with. However, it accepts also a project name as parameter to select a different Model Registry. Python import hsml # Create a connection connection = hsml . connection () # Get the model registry handle for the project's model registry mr = connection . get_model_registry ()","title":"Connection, Project and Model Registry"},{"location":"model-registry/quickstart/#models","text":"Assuming you have done some model training, and exported a model to a directory on a local file path, the model artifacts and additional metadata can now be saved to the Model Registry. See the example notebooks .","title":"Models"},{"location":"model-registry/quickstart/#creation","text":"Create a model named mnist . As you can see, you have the possibility to set parameters for the Model, such as the version number, or metrics which is set to attach model training metrics on the model. The Model Guide guides through the full configuration of Models. Python mnist_model_meta = mr . tensorflow . create_model ( name = \"mnist\" , version = 1 , metrics = { \"accuracy\" : 0.94 }, description = \"mnist model description\" ) Up to this point we have just created the metadata object representing the model. However, we haven't saved the model in the model registry yet. To do so, we can call the method save on the metadata object created in the cell above. The save method takes a single parameter which is the path to either the model file or the directory on the local filesystem containing multiple model files. Python mnist_model_meta . save ( \"/tmp/model_directory\" ) # or /tmp/model_file","title":"Creation"},{"location":"model-registry/quickstart/#retrieval","text":"If there were models previously created in your Model Registry, or you want to pick up where you left off before, you can retrieve and read models in a similar fashion as creating them: Using the Model Registry object, you can retrieve handles to the entities, such as models, in the Model Registry. By default, this will return the first version of an entity, if you want a more recent version, you need to specify the version. Python mnist_model_meta = mr . get_model ( 'mnist' , version = 1 ) # Download the model model_download_path = mnist_model_meta . download () # Load the model tf . saved_model . load ( model_download_path ) To seamlessly combine HSML with model serving components, the library makes it simple to query for the best performing model. In this instance, we get the best model version by querying for the model version with the highest accuracy metric attached. Python mnist_model_meta = mr . get_best_model ( 'mnist' , 'accuracy' , 'max' )","title":"Retrieval"},{"location":"model-serving/quickstart/","text":"Quickstart Guide # Hopsworks Model Serving enables models to be deployed on serving infrastructure and made securely accessible via a network endpoint. In this Quickstart Guide we are going to focus on how to configure the deployment of an existing model in the Model Registry and start making predictions with it via its network endpoint. HSML library # The Hopsworks Model Serving API is part of hsml ( H opswork s M achine L earning). The HSML library is Apache V2 licensed and available here . It currently comes with a Python SDK. If you want to use a deployed model endpoint from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within Model Serving. You can modify metadata by changing it in the metadata-objects and subsequently persisting them to Hopsworks. In fact, the Model Serving itself is also represented by an object. Furthermore, these objects have methods to deploy and undeploy models as well as perform actions on deployed models. Guide Notebooks # This guide is based on a series of notebooks . Connection, Project and Model Serving # The first step is to establish a connection with your Hopsworks instance and retrieve the object that represents the Model Serving you'll be working with. By default connection.get_model_serving() returns the Model Serving handle for the project you are working with. Python import hsml # Create a connection connection = hsml . connection () # Get the model serving handle for the project's model serving ms = connection . get_model_serving () Integration with the Model Registry # The Model Serving API integrates smoothly with the Model Registry API. Metadata objects such as Model can be used in multiple methods in the Model Serving library to simplify the code and avoid duplication of parameters. As a result, the easiest way to deploy an existing model is to call model.deploy() . For instance, the version with highest accuracy of a trained model can be deployed with the following lines. Python # Get the model registry handle for the project's model registry mr = connection . get_model_registry () # Get the version with highest accuracy model = mr . get_best_model ( 'mnist' , 'accuracy' , 'max' ) # Create a deployment deployment = model . deploy () To learn more about managing models with the Model Registry see the Model Registry documentation . Deployment # Assuming you have already created a model in the Model Registry, a deployment can now be created to prepare a model artifact for this model and make it accessible for running predictions behind a REST endpoint. A model artifact is a package containing all of the necessary files for the deployment of a model, including the model file and/or custom scripts for loading the model (the predictor script) or transforming the model inputs at inference time (the transformer script). See the example notebooks . Predictors and Transformers # Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions (see the Predictor Guide ). These inference requests can be transformed at inference time by configuring a transformer. Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model (see the Transformer Guide ). As an example, consider a model trained to detect unusual activity of users within a platform. Instead of sending all the recent activity records of a user each time we want to predict if the use is abnormal, we can simply send the user_id and build/enrich the model input in the transformer before sending it to the predictor. We enrich the model input by retrieving the most recent user activity from the Feature Store using the user_id as an entity key. Creation # Apart from the model.deploy() shortcut available in the model metadata object, deployments can be created in two additional ways: deploying a predictor or creating a deployment from scratch. To deploy a predictor, the Model Serving handle provides the ms.create_predictor() method to define the predictor metadata object which can be deployed using the predictor.deploy() method. Python # Define a predictor for a trained model predictor = ms . create_predictor ( model ) # Deploy the predictor deployment = predictor . deploy () Alternatively, deployments can be created and saved using the ms.create_deployment() and deployment.save() methods, respectively. Python # Define a predictor for a trained model predictor = ms . create_predictor ( model ) # Define a deployment deployment = ms . create_deployment ( predictor ) # Save the deployment in the Model Serving deployment . save () By default, the name of the deployment is inferred from the model name. To learn about the different parameters to configure a deployment see the Deployment Guide . Retrieval # If there were deployments previously created in your Model Serving, you can retrieve them by using the Model Serving handle. Moreover, you can retrieve a specific deployment by name or id. Python # Get all deployments deployments = ms . get_deployments () # Get deployment by name deployment = ms . get_deployment ( \"mnist\" ) # Get deployment by id deployment = ms . get_deployment_by_id ( 1 ) Start and stop deployment # The deployment metadata object provides methods for starting or stopping an existing deployment. Python # Start an existing deployment deployment . start () # Stop a running deployment deployment . stop () Get deployment status # To get the current state of a deployment, the deployment metadata object contains the method deployment.get_state() which provides information such as the status (e.g. running, stopped...) or inference endpoints to make prediction requests to. Python # Get the current state of a deployment current_state = deployment . get_state () # Check the deployment status print ( \" %s is %s \" % ( deployment . name , current_state . status )) # Print a full description of the deployment state current_state . describe () Make predictions using a deployment # Inference requests can be sent directly to the deployment using the deployment.predict() method available in the metadata object. Python data = { \"instances\" : model . input_example } # Make predictions with the deployed model predictions = deployment . predict ( data )","title":"Quickstart"},{"location":"model-serving/quickstart/#quickstart-guide","text":"Hopsworks Model Serving enables models to be deployed on serving infrastructure and made securely accessible via a network endpoint. In this Quickstart Guide we are going to focus on how to configure the deployment of an existing model in the Model Registry and start making predictions with it via its network endpoint.","title":"Quickstart Guide"},{"location":"model-serving/quickstart/#hsml-library","text":"The Hopsworks Model Serving API is part of hsml ( H opswork s M achine L earning). The HSML library is Apache V2 licensed and available here . It currently comes with a Python SDK. If you want to use a deployed model endpoint from outside Hopsworks, see our integration guides . The library is build around metadata-objects, representing entities within Model Serving. You can modify metadata by changing it in the metadata-objects and subsequently persisting them to Hopsworks. In fact, the Model Serving itself is also represented by an object. Furthermore, these objects have methods to deploy and undeploy models as well as perform actions on deployed models.","title":"HSML library"},{"location":"model-serving/quickstart/#guide-notebooks","text":"This guide is based on a series of notebooks .","title":"Guide Notebooks"},{"location":"model-serving/quickstart/#connection-project-and-model-serving","text":"The first step is to establish a connection with your Hopsworks instance and retrieve the object that represents the Model Serving you'll be working with. By default connection.get_model_serving() returns the Model Serving handle for the project you are working with. Python import hsml # Create a connection connection = hsml . connection () # Get the model serving handle for the project's model serving ms = connection . get_model_serving ()","title":"Connection, Project and Model Serving"},{"location":"model-serving/quickstart/#integration-with-the-model-registry","text":"The Model Serving API integrates smoothly with the Model Registry API. Metadata objects such as Model can be used in multiple methods in the Model Serving library to simplify the code and avoid duplication of parameters. As a result, the easiest way to deploy an existing model is to call model.deploy() . For instance, the version with highest accuracy of a trained model can be deployed with the following lines. Python # Get the model registry handle for the project's model registry mr = connection . get_model_registry () # Get the version with highest accuracy model = mr . get_best_model ( 'mnist' , 'accuracy' , 'max' ) # Create a deployment deployment = model . deploy () To learn more about managing models with the Model Registry see the Model Registry documentation .","title":"Integration with the Model Registry"},{"location":"model-serving/quickstart/#deployment","text":"Assuming you have already created a model in the Model Registry, a deployment can now be created to prepare a model artifact for this model and make it accessible for running predictions behind a REST endpoint. A model artifact is a package containing all of the necessary files for the deployment of a model, including the model file and/or custom scripts for loading the model (the predictor script) or transforming the model inputs at inference time (the transformer script). See the example notebooks .","title":"Deployment"},{"location":"model-serving/quickstart/#predictors-and-transformers","text":"Predictors are responsible for running a model server that loads a trained model, handles inference requests and returns predictions (see the Predictor Guide ). These inference requests can be transformed at inference time by configuring a transformer. Transformers are used to apply transformations on the model inputs before sending them to the predictor for making predictions using the model (see the Transformer Guide ). As an example, consider a model trained to detect unusual activity of users within a platform. Instead of sending all the recent activity records of a user each time we want to predict if the use is abnormal, we can simply send the user_id and build/enrich the model input in the transformer before sending it to the predictor. We enrich the model input by retrieving the most recent user activity from the Feature Store using the user_id as an entity key.","title":"Predictors and Transformers"},{"location":"model-serving/quickstart/#creation","text":"Apart from the model.deploy() shortcut available in the model metadata object, deployments can be created in two additional ways: deploying a predictor or creating a deployment from scratch. To deploy a predictor, the Model Serving handle provides the ms.create_predictor() method to define the predictor metadata object which can be deployed using the predictor.deploy() method. Python # Define a predictor for a trained model predictor = ms . create_predictor ( model ) # Deploy the predictor deployment = predictor . deploy () Alternatively, deployments can be created and saved using the ms.create_deployment() and deployment.save() methods, respectively. Python # Define a predictor for a trained model predictor = ms . create_predictor ( model ) # Define a deployment deployment = ms . create_deployment ( predictor ) # Save the deployment in the Model Serving deployment . save () By default, the name of the deployment is inferred from the model name. To learn about the different parameters to configure a deployment see the Deployment Guide .","title":"Creation"},{"location":"model-serving/quickstart/#retrieval","text":"If there were deployments previously created in your Model Serving, you can retrieve them by using the Model Serving handle. Moreover, you can retrieve a specific deployment by name or id. Python # Get all deployments deployments = ms . get_deployments () # Get deployment by name deployment = ms . get_deployment ( \"mnist\" ) # Get deployment by id deployment = ms . get_deployment_by_id ( 1 )","title":"Retrieval"},{"location":"model-serving/quickstart/#start-and-stop-deployment","text":"The deployment metadata object provides methods for starting or stopping an existing deployment. Python # Start an existing deployment deployment . start () # Stop a running deployment deployment . stop ()","title":"Start and stop deployment"},{"location":"model-serving/quickstart/#get-deployment-status","text":"To get the current state of a deployment, the deployment metadata object contains the method deployment.get_state() which provides information such as the status (e.g. running, stopped...) or inference endpoints to make prediction requests to. Python # Get the current state of a deployment current_state = deployment . get_state () # Check the deployment status print ( \" %s is %s \" % ( deployment . name , current_state . status )) # Print a full description of the deployment state current_state . describe ()","title":"Get deployment status"},{"location":"model-serving/quickstart/#make-predictions-using-a-deployment","text":"Inference requests can be sent directly to the deployment using the deployment.predict() method available in the metadata object. Python data = { \"instances\" : model . input_example } # Make predictions with the deployed model predictions = deployment . predict ( data )","title":"Make predictions using a deployment"}]}